{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSIunYoVTyfT"
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: primer3-py in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install primer3-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dsVQaGFwRFrF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import glob\n",
    "from collections import Counter\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from matplotlib import cm\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import copy\n",
    "import primer3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GhiFzoVT3aJ"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwtZO4inT5oz"
   },
   "source": [
    "## Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Yevi-htHT2vG"
   },
   "outputs": [],
   "source": [
    "def S3_to_sageMaker(sequence_type, variant_Name):\n",
    "    bucket='sars-cov-2-harry'\n",
    "    key = 'seq_and_seqName/{}_GISAID_{}.csv'.format(sequence_type, variant_Name)\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return obj['Body']\n",
    "\n",
    "\n",
    "def sys_path(basic_path):\n",
    "    variant_list = ['alpha', 'beta', 'gamma', 'delta', 'omicron']\n",
    "    # basic_path = './'\n",
    "    for variant in variant_list:\n",
    "        folder = os.path.exists(basic_path + 'Variant_virus/GISAID_' + variant)\n",
    "        if not folder:\n",
    "            os.makedirs(basic_path + 'Variant_virus/GISAID_' + variant)\n",
    "        for temp in ['Seq_and_SeqName', 'index', 'model', 'Final_result']:\n",
    "            folder = os.path.exists(basic_path + temp)\n",
    "            if not folder:\n",
    "                os.makedirs(basic_path + temp)\n",
    "        for forward_reverse in ['forward', 'reverse']:\n",
    "            for temp in ['filter', 'posPool', 'maxPool', 'filter_seq', 'featsVector', 'dataDNAFeatures', 'feature',\n",
    "                         'Seq_appearance', 'result']:\n",
    "                f = basic_path + variant + '/' + forward_reverse + '/' + temp\n",
    "                folder = os.path.exists(f)\n",
    "                if not folder:\n",
    "                    os.makedirs(f)\n",
    "            if forward_reverse == 'reverse':\n",
    "                for temp in ['model', 'amplicon_length', 'CG_Check', 'exist_primer_check', 'forward_primer',\n",
    "                             'second_data']:\n",
    "                    f = basic_path + variant + '/' + forward_reverse + '/' + temp\n",
    "                    folder = os.path.exists(f)\n",
    "                    if not folder:\n",
    "                        os.makedirs(f)\n",
    "                    if temp == 'CG_Check':\n",
    "                        for sub_temp in ['new_primers', 'exist_primers']:\n",
    "                            f = basic_path + variant + '/' + forward_reverse + '/' + temp + '/' + sub_temp\n",
    "                            folder = os.path.exists(f)\n",
    "                            if not folder:\n",
    "                                os.makedirs(f)\n",
    "\n",
    "\n",
    "def readFASTA(fa):\n",
    "    '''\n",
    "    :msg: read a xxx.fasta file\n",
    "    :param fa: --- {str} --- the path of the xxx.fasta file\n",
    "    :return: --- {dict} --- return a dictionary with key = seqName and value = sequence\n",
    "    '''\n",
    "    FA = open(fa)\n",
    "    seqDict = {}\n",
    "    for line in FA:\n",
    "        if line.startswith('>'):\n",
    "            seqName = line.replace('>', '').split()[0]\n",
    "            seqDict[seqName] = ''\n",
    "        else:\n",
    "            seqDict[seqName] += line.replace('\\n', '').strip()\n",
    "    FA.close()\n",
    "    return seqDict\n",
    "\n",
    "\n",
    "def readFASTA_iter(fa):\n",
    "    '''\n",
    "    :msg: read a xxx.fasta file\n",
    "    :param fa: --- {str} --- the path of the xxx.fasta file\n",
    "    :return: --- {generator} --- return a generator which gives each sequence name and sequence of the xxx.fasta file\n",
    "    '''\n",
    "    with open(fa, 'r') as FA:\n",
    "        seqName, seq = '', ''\n",
    "        while 1:\n",
    "            line = FA.readline()\n",
    "            line = line.strip('\\n')\n",
    "            if (line.startswith('>') or not line) and seqName:\n",
    "                yield ((seqName, seq))\n",
    "            if line.startswith('>'):\n",
    "                seqName = line[1:].split()[0]\n",
    "                seq = ''\n",
    "            else:\n",
    "                seq += line\n",
    "            if not line:\n",
    "                break\n",
    "\n",
    "\n",
    "def getSeq(fa, querySeqName, start=1, end=0):\n",
    "    '''\n",
    "    :msg: get a particular sequence of a xxx.fasta file\n",
    "    :param fa: --- {str} --- the path of the xxx.fasta file\n",
    "    :param querySeqName: --- {str} --- the name of the particular sequence\n",
    "    :param start: --- {int} --- the starting position of intercepting the sequence (defaults to 1)\n",
    "    :param end: --- {int} --- the ending position of intercepting the sequence (defaults to 0 / full length)\n",
    "    :return: --- {str} --- the sequence which intercepted\n",
    "    '''\n",
    "    if start < 0:\n",
    "        start = start + 1\n",
    "    for seqName, seq in readFASTA_iter(fa):\n",
    "        if querySeqName == seqName:\n",
    "            if end != 0:\n",
    "                returnSeq = seq[start - 1: end]\n",
    "                print('The start position and end position is {} / {}'.format(start - 1, end))\n",
    "            else:\n",
    "                returnSeq = seq[start - 1:]\n",
    "            return returnSeq\n",
    "\n",
    "\n",
    "def getReverseComplement(sequence):\n",
    "    '''\n",
    "    :msg: get the reverse cDNA of the RNA sequence\n",
    "    :param sequence: --- {str} --- a RNA sequence of the virus\n",
    "    :return: --- {str} --- the reverse cDNA sequence of the given RNA\n",
    "    '''\n",
    "    sequence = sequence.upper()\n",
    "    sequence = sequence.replace('A', 't')\n",
    "    sequence = sequence.replace('T', 'a')\n",
    "    sequence = sequence.replace('C', 'g')\n",
    "    sequence = sequence.replace('G', 'c')\n",
    "    return sequence.upper()[::-1]\n",
    "\n",
    "\n",
    "def getGC(sequence):\n",
    "    '''\n",
    "    :msg: get the GC content of a sequence\n",
    "    :param sequence: --- {str} --- a sequence of RNA\n",
    "    :return: --- {float} --- the GC content of sequence\n",
    "    '''\n",
    "    sequence = sequence.upper()\n",
    "    content = (sequence.count(\"G\") + sequence.count(\"C\")) / len(sequence)\n",
    "    return content\n",
    "\n",
    "\n",
    "def readSeqByWindow(sequence, winSize, stepSize):\n",
    "    '''\n",
    "    :msg: sliding window to read a sequence\n",
    "    :param sequence: --- {str} --- a sequence of RNA\n",
    "    :param winSize: --- {int} --- the Window size\n",
    "    :param stepSize: --- {int} --- the Step size\n",
    "    :return: --- {generator} --- return a generator which gives each sequence of the Window\n",
    "    '''\n",
    "    if stepSize <= 0:\n",
    "        return False\n",
    "    now = 0\n",
    "    seqLen = len(sequence)\n",
    "    while (now + winSize - stepSize < seqLen):\n",
    "        yield sequence[now:now + winSize]\n",
    "        now += stepSize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pmo7jT0BUFVw"
   },
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SPsAkBi2UHuX"
   },
   "outputs": [],
   "source": [
    "def oneHot(array, size):\n",
    "    output = []\n",
    "    for i in range(len(array)):\n",
    "        temp = np.zeros(size)\n",
    "        temp[int(array[i])] = 1\n",
    "        output.append(temp)\n",
    "    return np.array(output)\n",
    "\n",
    "\n",
    "# function to declare easily the weights only by shape\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# function to declare easily the bias only by shape\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def getBatch_run(data, labels, size, run, vector, sampleSize):\n",
    "    infLimit = run * size\n",
    "    supLimit = infLimit + size\n",
    "    if supLimit > len(data):\n",
    "        supLimit = len(data)\n",
    "    batch = []\n",
    "    for i in range(infLimit, supLimit):\n",
    "        batch.append(vector[i])\n",
    "    outData = []\n",
    "    outLabels = []\n",
    "    for i in range(len(batch)):\n",
    "        sample = np.zeros(sampleSize)\n",
    "        for j in range(0, len(data[batch[i]])):\n",
    "            if data[batch[i]][j] == 'C':\n",
    "                sample[j] = 0.25\n",
    "            elif data[batch[i]][j] == 'T':\n",
    "                sample[j] = 0.50\n",
    "            elif data[batch[i]][j] == 'G':\n",
    "                sample[j] = 0.75\n",
    "            elif data[batch[i]][j] == 'A':\n",
    "                sample[j] = 1.0\n",
    "            else:\n",
    "                sample[j] = 0.0\n",
    "        outData.append(sample)\n",
    "        outLabels.append(labels[batch[i]])\n",
    "    return np.array(outData), np.array(outLabels)\n",
    "\n",
    "\n",
    "def getBatch(data, labels, size, sampleSize):\n",
    "    index = []\n",
    "    for i in range(len(data)):\n",
    "        index.append(i)\n",
    "    batch = random.sample(index, size)\n",
    "    outData = []\n",
    "    outLabels = []\n",
    "    for i in range(len(batch)):\n",
    "        sample = np.zeros(sampleSize)\n",
    "        for j in range(0, len(data[batch[i]])):\n",
    "            if (data[batch[i]][j] == 'C'):\n",
    "                sample[j] = 0.25\n",
    "            elif (data[batch[i]][j] == 'T'):\n",
    "                sample[j] = 0.50\n",
    "            elif (data[batch[i]][j] == 'G'):\n",
    "                sample[j] = 0.75\n",
    "            elif (data[batch[i]][j] == 'A'):\n",
    "                sample[j] = 1.0\n",
    "            else:\n",
    "                sample[j] = 0.0\n",
    "        outData.append(sample)\n",
    "        outLabels.append(labels[batch[i]])\n",
    "    return np.array(outData), np.array(outLabels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvQ90NQ1UYVG"
   },
   "source": [
    "## Other Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qaULgPlUZU7"
   },
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oqNEkl-4UcAB"
   },
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    print('------------------ Processing get_data ------------------')\n",
    "    variant_list = ['alpha', 'beta', 'gamma', 'delta', 'omicron']\n",
    "\n",
    "    for Name in variant_list:\n",
    "        print('\\nNow running the --- {} --- variant\\n'.format(Name))\n",
    "        files = glob.glob(path + 'Variant_virus/GISAID_' + Name + '/*')\n",
    "        seqName_list, seq_list = [], []\n",
    "        file_count, data_count = 0, 0\n",
    "        for file in files:\n",
    "            file_count += 1\n",
    "            for seqName, seq in readFASTA_iter(file):\n",
    "                data_count += 1\n",
    "                seqName_list.append(seqName)\n",
    "                seq_list.append(seq)\n",
    "                if data_count % 1000 == 0:\n",
    "                    print('{} ... No.{} file with {} sequence'.format(Name, file_count, data_count))\n",
    "\n",
    "        print('\\n{} : {} sequence\\n'.format(Name, len(seqName_list)))\n",
    "        print('Saving the seqName_list...\\n')\n",
    "        pd.DataFrame(seqName_list).to_csv(path + 'Seq_and_SeqName/seqName_GISAID_' + Name + '.csv',\n",
    "                                          header=None, index=None)\n",
    "        print('\\nSaving the seq_list...')\n",
    "        pd.DataFrame(seq_list).to_csv(path + 'Seq_and_SeqName/seq_GISAID_' + Name + '.csv',\n",
    "                                      header=None, index=None)\n",
    "\n",
    "\n",
    "def mixed_data(basic_path, delta_num, other_percent=1):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v3_Dataset/'\n",
    "    variant_list = ['alpha', 'beta', 'gamma', 'delta', 'omicron']\n",
    "\n",
    "    vectorSize = 0\n",
    "    delta_num = delta_num * 2\n",
    "    mix_seqName = []\n",
    "    mix_sequence = []\n",
    "    mix_label = []\n",
    "\n",
    "    for Name in variant_list:\n",
    "        seqName_file = S3_to_sageMaker('seqName', Name)\n",
    "        seqName = pd.read_csv(seqName_file, header=None).values.ravel()  # get the sequence name\n",
    "\n",
    "        seq_file = S3_to_sageMaker('seq', Name)\n",
    "        sequence = pd.read_csv(seq_file, header=None).values.ravel()  # get the sequence\n",
    "\n",
    "        for i in range(len(sequence)):\n",
    "            if len(sequence[i]) > vectorSize:\n",
    "                vectorSize = len(sequence[i])\n",
    "        print('{}    vector size : {}'.format(Name, vectorSize))\n",
    "\n",
    "        rand = np.random.randint(100000)\n",
    "\n",
    "        if Name == 'alpha':\n",
    "            label = 1\n",
    "            np.random.seed(rand)\n",
    "            np.random.shuffle(seqName)\n",
    "            mix_seqName.append(seqName[:int(delta_num * other_percent)])\n",
    "\n",
    "            np.random.seed(rand)\n",
    "            np.random.shuffle(sequence)\n",
    "            mix_sequence.append(sequence[:int(delta_num * other_percent)])\n",
    "\n",
    "            seq_labels = np.array([label for x in range(len(seqName[:int(delta_num * other_percent)]))])\n",
    "            np.random.seed(rand)\n",
    "            np.random.shuffle(seq_labels)\n",
    "            mix_label.append(seq_labels)\n",
    "\n",
    "        elif Name == 'beta':\n",
    "            label = 2\n",
    "            np.random.seed(rand)\n",
    "            np.random.shuffle(seqName)\n",
    "            mix_seqName.append(seqName[:int(delta_num * other_percent)])\n",
    "\n",
    "            np.random.seed(rand)\n",
    "            np.random.shuffle(sequence)\n",
    "            mix_sequence.append(sequence[:int(delta_num * other_percent)])\n",
    "\n",
    "            seq_labels = np.array([label for x in range(len(seqName[:int(delta_num * other_percent)]))])\n",
    "            np.random.seed(rand)\n",
    "            np.random.shuffle(seq_labels)\n",
    "            mix_label.append(seq_labels)\n",
    "\n",
    "        elif Name == 'gamma':\n",
    "            label = 3\n",
    "            np.random.seed(rand)\n",
    "            np.random.shuffle(seqName)\n",
    "            mix_seqName.append(seqName[:int(delta_num * other_percent)])\n",
    "\n",
    "            np.random.seed(rand)\n",
    "            np.random.shuffle(sequence)\n",
    "            mix_sequence.append(sequence[:int(delta_num * other_percent)])\n",
    "\n",
    "            seq_labels = np.array([label for x in range(len(seqName[:int(delta_num * other_percent)]))])\n",
    "            np.random.seed(rand)\n",
    "            np.random.shuffle(seq_labels)\n",
    "            mix_label.append(seq_labels)\n",
    "\n",
    "        elif Name == 'delta':\n",
    "            label = 4\n",
    "            np.random.seed(rand)\n",
    "            np.random.shuffle(seqName)\n",
    "            mix_seqName.append(seqName[:delta_num])\n",
    "\n",
    "            np.random.seed(rand)\n",
    "            np.random.shuffle(sequence)\n",
    "            mix_sequence.append(sequence[:delta_num])\n",
    "\n",
    "            seq_labels = np.array([label for x in range(len(seqName[:delta_num]))])\n",
    "            np.random.seed(rand)\n",
    "            np.random.shuffle(seq_labels)\n",
    "            mix_label.append(seq_labels)\n",
    "\n",
    "        elif Name == 'omicron':\n",
    "            label = 0\n",
    "            np.random.seed(rand)\n",
    "            np.random.shuffle(seqName)\n",
    "            mix_seqName.append(seqName[:delta_num])\n",
    "\n",
    "            np.random.seed(rand)\n",
    "            np.random.shuffle(sequence)\n",
    "            mix_sequence.append(sequence[:delta_num])\n",
    "\n",
    "            seq_labels = np.array([label for x in range(len(seqName[:delta_num]))])\n",
    "            np.random.seed(rand)\n",
    "            np.random.shuffle(seq_labels)\n",
    "            mix_label.append(seq_labels)\n",
    "\n",
    "    pd.DataFrame(mix_seqName).to_csv(basic_path + 'mix_seqName.csv', header=None, index=None)\n",
    "    pd.DataFrame(mix_sequence).to_csv(basic_path + 'mix_mix_sequence.csv', header=None, index=None)\n",
    "    pd.DataFrame(mix_label).to_csv(basic_path + 'mix_label.csv', header=None, index=None)\n",
    "\n",
    "    return vectorSize\n",
    "\n",
    "\n",
    "def train_valid_data(basic_path, n_splits=2):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v3_Dataset/'\n",
    "\n",
    "    mix_seqName = pd.read_csv(basic_path + 'mix_seqName.csv', header=None).values.ravel()\n",
    "    mix_sequence = pd.read_csv(basic_path + 'mix_mix_sequence.csv', header=None).values.ravel()\n",
    "    mix_label = pd.read_csv(basic_path + 'mix_label.csv', header=None).values.ravel()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    # skf.get_n_splits(mix_sequence, mix_label)\n",
    "    for train_index, test_index in skf.split(mix_sequence, mix_label):\n",
    "        mix_sequence_train, mix_sequence_test = mix_sequence[train_index], mix_sequence[test_index]\n",
    "        mix_seqName_train, mix_seqName_test = mix_seqName[train_index], mix_seqName[test_index]\n",
    "        mix_label_train, mix_label_test = mix_label[train_index], mix_label[test_index]\n",
    "\n",
    "    pd.DataFrame(mix_sequence_train).to_csv(basic_path + 'index/train_sequence.csv', header=None, index=None)\n",
    "    pd.DataFrame(mix_sequence_test).to_csv(basic_path + 'index/valid_sequence.csv', header=None, index=None)\n",
    "\n",
    "    pd.DataFrame(mix_seqName_train).to_csv(basic_path + 'index/train_seqName.csv', header=None, index=None)\n",
    "    pd.DataFrame(mix_seqName_test).to_csv(basic_path + 'index/valid_seqName.csv', header=None, index=None)\n",
    "\n",
    "    pd.DataFrame(mix_label_train).to_csv(basic_path + 'index/train_label.csv', header=None, index=None)\n",
    "    pd.DataFrame(mix_label_test).to_csv(basic_path + 'index/valid_label.csv', header=None, index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMCPAtsIUfDm"
   },
   "source": [
    "### Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KCau9avTUhSV"
   },
   "outputs": [],
   "source": [
    "\n",
    "def posPool(path, vectorSize, max_Pooling_window_size):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v5_Dataset/'\n",
    "    files = glob.glob(path + 'filter/*.csv')\n",
    "    numberWindows = int(vectorSize / max_Pooling_window_size) + 1\n",
    "    for file in files:\n",
    "        filterIndex = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "        data = pd.read_csv(file, header=None).values\n",
    "\n",
    "        sizeData = np.shape(data)\n",
    "\n",
    "        print(sizeData)\n",
    "\n",
    "        maxPool = np.zeros(shape=(sizeData[0], numberWindows))\n",
    "        posPool = np.zeros(shape=(sizeData[0], numberWindows))\n",
    "\n",
    "        for i in range(0, sizeData[0]):\n",
    "            maxPool_windowSize = max_Pooling_window_size\n",
    "            pad_left_HPool = 0\n",
    "            max = -1e6\n",
    "            index = pad_left_HPool\n",
    "            position = -1\n",
    "            indexMax = 0\n",
    "            for j in range(0, sizeData[1]):\n",
    "                if data[i][j] > max:\n",
    "                    max = data[i][j]\n",
    "                    position = j\n",
    "                index = index + 1\n",
    "                if index == maxPool_windowSize or j == sizeData[1] - 1:\n",
    "                    maxPool[i][indexMax] = max\n",
    "                    posPool[i][indexMax] = position\n",
    "                    max = -1e6\n",
    "                    position = -1\n",
    "                    index = 0\n",
    "                    indexMax = indexMax + 1\n",
    "\n",
    "        pd.DataFrame(maxPool).to_csv(path + 'maxPool/maxPool_' + str(filterIndex) + '.csv', header=None, index=None)\n",
    "        pd.DataFrame(posPool).to_csv(path + 'posPool/posPool_' + str(filterIndex) + '.csv', header=None, index=None)\n",
    "\n",
    "    return numberWindows\n",
    "\n",
    "\n",
    "def posPool_top(path, vectorSize, max_Pooling_window_size):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v5_Dataset/'\n",
    "    files = glob.glob(path + 'filter/*.csv')\n",
    "    numberWindows = int(vectorSize / max_Pooling_window_size) + 1\n",
    "    for file in files:\n",
    "        filterIndex = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "        data = pd.read_csv(file, header=None).values\n",
    "\n",
    "        sizeData = np.shape(data)\n",
    "\n",
    "        print(sizeData)\n",
    "\n",
    "        maxPool = np.zeros(shape=(sizeData[0], numberWindows))\n",
    "        posPool = np.zeros(shape=(sizeData[0], numberWindows))\n",
    "\n",
    "        for i in range(0, sizeData[0]):\n",
    "            print(i)\n",
    "\n",
    "            temp = list(data[i])\n",
    "            t = copy.deepcopy(temp)\n",
    "\n",
    "            max_number = []\n",
    "            max_index = []\n",
    "            for _ in range(numberWindows):\n",
    "                number = np.max(t)\n",
    "                index = t.index(number)\n",
    "                t[index] = 0\n",
    "                max_number.append(number)\n",
    "                max_index.append(index)\n",
    "\n",
    "            for j in range(0, numberWindows):\n",
    "                maxPool[i][j] = max_number[j]\n",
    "                posPool[i][j] = max_index[j]\n",
    "\n",
    "        pd.DataFrame(maxPool).to_csv(path + 'maxPool/maxPool_' + str(filterIndex) + '.csv', header=None, index=None)\n",
    "        pd.DataFrame(posPool).to_csv(path + 'posPool/posPool_' + str(filterIndex) + '.csv', header=None, index=None)\n",
    "\n",
    "    return numberWindows\n",
    "\n",
    "\n",
    "def posPool_combination(path, vectorSize, max_Pooling_window_size, top_in_window):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v5_Dataset/'\n",
    "    files = glob.glob(path + 'filter/*.csv')\n",
    "    numberWindows = (int(vectorSize / max_Pooling_window_size) + 1) * top_in_window\n",
    "    for file in files:\n",
    "        filterIndex = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "        data = pd.read_csv(file, header=None).values\n",
    "\n",
    "        sizeData = np.shape(data)\n",
    "\n",
    "        print(sizeData)\n",
    "\n",
    "        maxPool = np.zeros(shape=(sizeData[0], numberWindows))\n",
    "        posPool = np.zeros(shape=(sizeData[0], numberWindows))\n",
    "\n",
    "        for i in range(0, sizeData[0]):\n",
    "            print(i)\n",
    "            maxPool_windowSize = max_Pooling_window_size\n",
    "            pad_left_HPool = 0\n",
    "            max = -1e6\n",
    "            index = pad_left_HPool\n",
    "            position = -1\n",
    "            indexMax = 0\n",
    "\n",
    "            max_number = []\n",
    "            max_index = []\n",
    "\n",
    "            temp_value = []\n",
    "\n",
    "            loop = 0\n",
    "            for j in range(0, sizeData[1]):\n",
    "                temp_value.append(data[i][j])\n",
    "                index = index + 1\n",
    "                if index == maxPool_windowSize or j == sizeData[1] - 1:\n",
    "                    t = copy.deepcopy(temp_value)\n",
    "\n",
    "                    for _ in range(top_in_window):\n",
    "                        number = np.max(t)\n",
    "                        index = t.index(number)\n",
    "                        t[index] = 0\n",
    "                        max_number.append(number)\n",
    "                        max_index.append(index + loop * max_Pooling_window_size)\n",
    "\n",
    "                    loop += 1\n",
    "                    temp_value = []\n",
    "                    index = 0\n",
    "\n",
    "            for j in range(numberWindows):\n",
    "                maxPool[i][j] = max_number[j]\n",
    "                posPool[i][j] = max_index[j]\n",
    "\n",
    "        pd.DataFrame(maxPool).to_csv(path + 'maxPool/maxPool_' + str(filterIndex) + '.csv', header=None, index=None)\n",
    "        pd.DataFrame(posPool).to_csv(path + 'posPool/posPool_' + str(filterIndex) + '.csv', header=None, index=None)\n",
    "\n",
    "    return numberWindows\n",
    "\n",
    "       \n",
    "def creatFeatVector(path, numberWindows, vectorSize, numberFilters):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v5_Dataset/'\n",
    "    pos_path = path + 'posPool/'\n",
    "    path_seq = path + 'filter_seq/'\n",
    "    files = glob.glob(pos_path + '/*.csv')\n",
    "\n",
    "    # Parameters\n",
    "    # numberFilters = 21\n",
    "\n",
    "    p_value = 0\n",
    "    if numberFilters % 2 == 0:\n",
    "        p_value = 1\n",
    "        numberFilters += 1\n",
    "    \n",
    "    padding = int((numberFilters - 1) / 2)\n",
    "\n",
    "    for file in files:\n",
    "        filterIndex = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "        print('Processing...   Loop 1 -- Index {}'.format(filterIndex))\n",
    "\n",
    "        posMatrix = pd.read_csv(file, header=None).values\n",
    "        matrix = pd.read_csv(path_seq + 'filter_seq.csv', header=None).values.ravel()\n",
    "\n",
    "        outData = []\n",
    "        for i in range(len(matrix)):\n",
    "            sample = np.zeros(vectorSize)\n",
    "            for j in range(len(matrix[i])):\n",
    "                if matrix[i][j] == 'C':\n",
    "                    sample[j] = 0.25\n",
    "                elif matrix[i][j] == 'T':\n",
    "                    sample[j] = 0.50\n",
    "                elif matrix[i][j] == 'G':\n",
    "                    sample[j] = 0.75\n",
    "                elif matrix[i][j] == 'A':\n",
    "                    sample[j] = 1.0\n",
    "                else:\n",
    "                    sample[j] = 0.0\n",
    "            outData.append(sample)\n",
    "\n",
    "        matrix = np.array(outData)\n",
    "        sizePosMatrix = np.shape(posMatrix)\n",
    "        dataDNA = [[0 for i in range(numberWindows * numberFilters)] for j in range(sizePosMatrix[0])]\n",
    "        sizeDNAMatrix = np.shape(matrix)\n",
    "\n",
    "        for i in range(sizePosMatrix[0]):\n",
    "            temp = ((matrix[i]))\n",
    "            for j in range(sizePosMatrix[1]):\n",
    "                coef = int(posMatrix[i][j])\n",
    "                for k in range(padding + 1):\n",
    "                    if (coef + k) < len(temp):\n",
    "                        dataDNA[i][j * numberFilters + padding + k] = temp[coef + k]\n",
    "                    if (coef - k) >= 0 and (coef - k) < len(temp):\n",
    "                        dataDNA[i][j * numberFilters + padding - k] = temp[coef - k]\n",
    "\n",
    "        dataDNAString = [[0 for i in range(numberWindows * numberFilters)] for j in range(sizePosMatrix[0])]\n",
    "\n",
    "        for i in range(sizePosMatrix[0]):\n",
    "            for j in range(numberWindows * numberFilters):\n",
    "                if dataDNA[i][j] == 0.25:\n",
    "                    dataDNAString[i][j] = \"C\"\n",
    "                elif dataDNA[i][j] == 0.50:\n",
    "                    dataDNAString[i][j] = \"T\"\n",
    "                elif dataDNA[i][j] == 0.75:\n",
    "                    dataDNAString[i][j] = \"G\"\n",
    "                elif dataDNA[i][j] == 1.00:\n",
    "                    dataDNAString[i][j] = \"A\"\n",
    "                else:\n",
    "                    dataDNAString[i][j] = \"N\"\n",
    "\n",
    "        dataDNAFeatures = [[0 for i in range(numberWindows)] for j in range(sizePosMatrix[0])]\n",
    "        for i in range(sizePosMatrix[0]):\n",
    "            for j in range(numberWindows):\n",
    "                dataDNAFeatures[i][j] = str(\"\")\n",
    "\n",
    "        for i in range(sizePosMatrix[0]):\n",
    "            indexFeature = 0\n",
    "            feature = 0\n",
    "            for j in range(numberWindows * numberFilters):\n",
    "                dataDNAFeatures[i][feature] = str(dataDNAFeatures[i][feature]) + str(dataDNAString[i][j])\n",
    "                indexFeature = indexFeature + 1\n",
    "                if indexFeature == numberFilters:\n",
    "                    feature = feature + 1\n",
    "                    indexFeature = 0\n",
    "\n",
    "        featsVector = []\n",
    "        for i in range(sizePosMatrix[0]):\n",
    "            for j in range(numberWindows):\n",
    "                count = featsVector.count(dataDNAFeatures[i][j])\n",
    "                if count == 0:\n",
    "                    if \"N\" not in dataDNAFeatures[i][j]:\n",
    "                        featsVector.append(dataDNAFeatures[i][j])\n",
    "        \n",
    "        new_featsVector = ['NNNNNNNNNNNNNNNNNNNNN']\n",
    "        if p_value == 1:\n",
    "            for feats in featsVector:\n",
    "                new_feats = ''.join(list(feats)[:-1])\n",
    "                new_featsVector.append(new_feats)\n",
    "\n",
    "        pd.DataFrame(new_featsVector).to_csv(path + 'featsVector/featsVector_' + str(filterIndex) + '.csv',\n",
    "                                         header=None, index=None)\n",
    "        # pd.DataFrame(dataDNAFeatures).to_csv(path + 'dataDNAFeatures/dataDNAFeatures_' + str(filterIndex) + '.csv',\n",
    "        #                                      header=None, index=None)\n",
    "\n",
    "\n",
    "\n",
    "def getFeature(basic_path, path, variant_Name, Number):\n",
    "    files = glob.glob(path + 'featsVector/*.csv')\n",
    "    seq_file = S3_to_sageMaker('seq', variant_Name)\n",
    "    sequences = pd.read_csv(seq_file, header=None).values.ravel()  # get the sequence\n",
    "    np.random.shuffle(sequences)\n",
    "    sequences = sequences[:Number]\n",
    "\n",
    "    for file in files:\n",
    "        filterIndex = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "        print('Processing...   Loop 1 -- Index {}'.format(filterIndex))\n",
    "\n",
    "        vector = pd.read_csv(file, header=None).values.ravel()\n",
    "        print('Sequence Size: {}'.format(len(sequences)))\n",
    "        print('featVector Size: {}'.format(len(vector)))\n",
    "\n",
    "        featureList = []\n",
    "        count = 0\n",
    "        for seq in sequences:\n",
    "            count += 1\n",
    "            if count % 100 == 0:\n",
    "                print('Calculating the features in sequence No.{}      with {} Total'.format(count, len(sequences)))\n",
    "            for feature in vector:\n",
    "                if feature in seq:\n",
    "                    feature_count = seq.count(feature)\n",
    "                    if feature_count == 1 and feature not in featureList:\n",
    "                        featureList.append(feature)\n",
    "        if len(featureList):\n",
    "            pd.DataFrame(featureList).to_csv(path + 'feature/features_' + str(filterIndex) + '.csv', header=None, index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkZToO1WUmDN"
   },
   "source": [
    "### Appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RdbYrRnwUx1a"
   },
   "outputs": [],
   "source": [
    "def sameFeature(path):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v3_Dataset/'\n",
    "\n",
    "    feature_files = glob.glob(path + 'feature/*')\n",
    "\n",
    "    p_value = 0\n",
    "    for file in feature_files:\n",
    "        if p_value == 0:\n",
    "            feature_data = list(pd.read_csv(file, header=None).values.ravel())\n",
    "            p_value = 1\n",
    "        elif p_value == 1:\n",
    "            feature_data_new = list(pd.read_csv(file, header=None).values.ravel())\n",
    "            for var in feature_data_new:\n",
    "                feature_data.append(var)\n",
    "\n",
    "    CountFeature_dict = Counter(feature_data)\n",
    "\n",
    "    RepeatList = []\n",
    "    nonRepeatList = list(set(feature_data))\n",
    "    new_features = []\n",
    "    for feature in nonRepeatList:\n",
    "        if 'C' in feature and 'G' in feature:\n",
    "            CountFeature_dict = Counter(feature)\n",
    "            CG_content = CountFeature_dict.get('C') + CountFeature_dict.get('G')\n",
    "\n",
    "            if CG_content > len(feature) * 0.3 and CG_content < len(feature) * 0.7:\n",
    "                new_features.append(feature)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    nonRepeatList = list(set(new_features))\n",
    "\n",
    "    for key in CountFeature_dict.keys():\n",
    "        value = CountFeature_dict.get(key)\n",
    "        if value != 1:\n",
    "            RepeatList.append(key)\n",
    "\n",
    "    pd.DataFrame(RepeatList).to_csv(path + 'Repeat_feature_List.csv', header=None, index=None)\n",
    "    pd.DataFrame(nonRepeatList).to_csv(path + 'nonRepeat_feature_List.csv', header=None, index=None)\n",
    "\n",
    "\n",
    "def  get_appearance(feature_file, seq_file, high_type=1, accuracy=0.95):\n",
    "    '''\n",
    "    :msg: get the accuracy of a feature sequence higher than 95% or lower than 5%\n",
    "    :param feature_file: --- {str} --- the path of the feature sequence file\n",
    "    :param seq_file: --- {str} --- the path of the sequence file\n",
    "    :param high_type: --- {int} --- a compare param (defaults to 1)\n",
    "    :param accuracy: --- {float} --- the limitation of the compare (defaults to 0.95)\n",
    "    :return: the DataFrame with the appearance of each feature sequence\n",
    "    '''\n",
    "    features = pd.read_csv(feature_file, header=None).values.ravel()  # get 3827 features from the CNN model\n",
    "\n",
    "    # features = pd.read_csv(feature_file, header=None)\n",
    "    # features = features[1:][1].values.ravel()\n",
    "\n",
    "    # seq = pd.read_csv(seq_file, header=None).values.ravel()\n",
    "    seq = pd.read_csv(seq_file, header=None).values.ravel()\n",
    "\n",
    "    np.random.shuffle(seq)\n",
    "    seq = pd.DataFrame(seq[:5000])\n",
    "\n",
    "    total_num = len(seq)\n",
    "    featureDic = {}\n",
    "    count = 1\n",
    "    for feature in features:\n",
    "        print('feature ---- No. {}      with {} Total'.format(count, len(features)))\n",
    "        count += 1\n",
    "\n",
    "        count_feature = seq[0].apply(lambda x: x.count(feature))\n",
    "        available_count = len(count_feature[count_feature == 1])\n",
    "\n",
    "        if high_type == 1 and available_count / total_num >= accuracy:\n",
    "            featureDic[feature] = available_count / total_num\n",
    "        elif high_type != 1 and available_count / total_num <= 1 - accuracy:\n",
    "            featureDic[feature] = available_count / total_num\n",
    "\n",
    "    featureDF = pd.Series(featureDic).to_frame()\n",
    "    return featureDF\n",
    "\n",
    "\n",
    "def calculateAppearance(basic_path, path, variant_Name):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v3_Dataset/'\n",
    "\n",
    "    files = glob.glob(basic_path + 'Variant_virus/*')\n",
    "    p_value = 0\n",
    "\n",
    "    order = [3, 1, 0, 2, 4]  # original order = ['alpha', 'beta', 'gamma', 'delta', 'omicron']\n",
    "    for i in order:\n",
    "        file = files[i]\n",
    "        file = file.split('/')[-1]\n",
    "        save_fileName = file.split('.')[0].replace('GISAID_', '')\n",
    "        # save_Fasta(path, file, save_fileName)\n",
    "\n",
    "        # feature_file = path + 'Repeat_feature_List.csv'\n",
    "        feature_file = path + 'nonRepeat_feature_List.csv'\n",
    "        seq_file = S3_to_sageMaker('seq', save_fileName)\n",
    "\n",
    "        print('Get the appearance in  -- {} --  virus'.format(save_fileName))\n",
    "        if variant_Name == 'omicron':\n",
    "            if save_fileName == 'omicron':\n",
    "                if p_value == 0:\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=0.80)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "                    p_value = 1\n",
    "\n",
    "                elif p_value != 0:\n",
    "                    feature_file = path + 'result/temp_feature.csv'\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=0.80)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "            elif save_fileName == 'delta':\n",
    "                if p_value == 0:\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=-1)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "                    p_value = 1\n",
    "\n",
    "                elif p_value != 0:\n",
    "                    feature_file = path + 'result/temp_feature.csv'\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=-1)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "            else:\n",
    "                if p_value == 0:\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=0, accuracy=0.90)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "                    p_value = 1\n",
    "\n",
    "                elif p_value != 0:\n",
    "                    feature_file = path + 'result/temp_feature.csv'\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=0, accuracy=0.90)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "        else:\n",
    "            if save_fileName == 'omicron':\n",
    "                if p_value == 0:\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=-1)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "                    p_value = 1\n",
    "\n",
    "                elif p_value != 0:\n",
    "                    feature_file = path + 'result/temp_feature.csv'\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=-1)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "            elif save_fileName == variant_Name:\n",
    "                if p_value == 0:\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=0.95)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "                    p_value = 1\n",
    "\n",
    "                elif p_value != 0:\n",
    "                    feature_file = path + 'result/temp_feature.csv'\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=0.95)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "            else:\n",
    "                if p_value == 0:\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=0, accuracy=0.95)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "                    p_value = 1\n",
    "\n",
    "                elif p_value != 0:\n",
    "                    feature_file = path + 'result/temp_feature.csv'\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=0, accuracy=0.95)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "\n",
    "\n",
    "def commonFeatureAppearance(path):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v3_Dataset/'\n",
    "\n",
    "    files = glob.glob(path + 'Seq_appearance/*')\n",
    "\n",
    "    Name_list = []\n",
    "    p_value = 0\n",
    "    for file in files:\n",
    "        file_name = file.split('/')[-1].split('.')[0]\n",
    "        Name_list.append(file_name)\n",
    "\n",
    "        if p_value == 0:\n",
    "            data = pd.read_csv(file, header=None)\n",
    "            data = data[0][data[0].notna()].values.ravel()\n",
    "            p_value = 1\n",
    "        elif p_value == 1:\n",
    "            data_new = pd.read_csv(file, header=None)\n",
    "            data_new = data_new[0][data_new[0].notna()].values.ravel()\n",
    "\n",
    "            data = [x for x in data if x in data_new]\n",
    "\n",
    "    Appearance_list = []\n",
    "    for seq in data:\n",
    "        print('\\nFor the sequence: {}\\n'.format(seq))\n",
    "        temp_list = []\n",
    "        for file in files:\n",
    "            file_name = file.split('/')[-1].split('.')[0]\n",
    "            data_appearance = pd.read_csv(file, header=None)\n",
    "            appearance = data_appearance[data_appearance[0] == seq][1].values.ravel()[0]\n",
    "            # print('{} :  {}'.format(file_name, appearance))\n",
    "            temp_list.append(appearance)\n",
    "        Appearance_list.append(temp_list)\n",
    "\n",
    "    Appearance_DF = pd.DataFrame(Appearance_list)\n",
    "    Appearance_DF.index = data\n",
    "    Appearance_DF.columns = Name_list\n",
    "\n",
    "    Appearance_DF = Appearance_DF[\n",
    "        ['feature_alpha', 'feature_beta', 'feature_gamma', 'feature_delta', 'feature_omicron']]\n",
    "\n",
    "    Appearance_DF = Appearance_DF.sort_values(by=['feature_omicron'], ascending=[False])\n",
    "    Appearance_DF.to_csv(path + 'Appearance_DataFrame.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqbXGuB6UxU3"
   },
   "source": [
    "### Forward --> Reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Cpl56YohU3Dy"
   },
   "outputs": [],
   "source": [
    "def get_forward_primers(path):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v5_Dataset/'\n",
    "    print('------------------ Processing get_data ------------------')\n",
    "\n",
    "    files = glob.glob(path + 'forward_primer/*')\n",
    "    forward_primer_list = []\n",
    "    count = 0\n",
    "    print('Start to collect forward primers...')\n",
    "    for file in files:\n",
    "        primers = pd.read_csv(file)['Unnamed: 0'].values.ravel()\n",
    "        for forward_primer in primers:\n",
    "            count += 1\n",
    "            forward_primer_list.append(forward_primer)\n",
    "    print('Saving the forward primers...'.format(len(forward_primer_list), count))\n",
    "    forward_primer_list = list(set(forward_primer_list))\n",
    "    print('Finished !     ({}/{})\\n'.format(len(forward_primer_list), count))\n",
    "    pd.DataFrame(forward_primer_list).to_csv(path + 'forward_primer.csv', header=None, index=None)\n",
    "\n",
    "\n",
    "def CG_content_chect(path, min_CG=0.35, max_CG=0.65):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v5_Dataset/'\n",
    "    print('Checking the CG content of the forward primers obtained....')\n",
    "\n",
    "    features = pd.read_csv(path + 'forward_primer.csv', header=None).values.ravel()\n",
    "\n",
    "    new_features = []\n",
    "    for feature in features:\n",
    "        if 'C' in feature and 'G' in feature:\n",
    "            CountFeature_dict = Counter(feature)\n",
    "            CG_content = CountFeature_dict.get('C') + CountFeature_dict.get('G')\n",
    "\n",
    "            if len(feature) * min_CG < CG_content < len(feature) * max_CG:\n",
    "                new_features.append(feature)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    Feature_List = list(set(new_features))\n",
    "    print('Finished !     ({}/{})        min={}  max={}'.format(len(Feature_List), len(features), min_CG, max_CG))\n",
    "    pd.DataFrame(Feature_List).to_csv(path + 'forward_primer_CG_check.csv', header=None, index=None)\n",
    "\n",
    "\n",
    "def get_after_primer_data(path, forward_primer, sequence):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v5_Dataset/'\n",
    "\n",
    "    # seq_file = S3_to_sageMaker('seq', variant_Name)\n",
    "    # sequence = pd.read_csv(seq_file, header=None).values.ravel()  # get the sequence\n",
    "    # primers = pd.read_csv(path + 'forward_primer.csv', header=None).values.ravel()\n",
    "\n",
    "    # count = 0\n",
    "    # for forward_primer in primers:\n",
    "        # count += 1\n",
    "    print('\\nNow processing with forward primer:  {}\\n'.format(forward_primer))\n",
    "    in_count, out_count = 0, 0\n",
    "    second_half_list = []\n",
    "\n",
    "    np.random.shuffle(sequence)\n",
    "    sequence = sequence[:6000]\n",
    "    for i in range(len(sequence)):\n",
    "        if i % 5000 == 0:\n",
    "            print('No. {} sequence... with {} in the sequence     /     with {} out of the sequence'.format(i,\n",
    "                                                                                                            in_count,\n",
    "                                                                                                            out_count))\n",
    "        if forward_primer in sequence[i]:\n",
    "            in_count += 1\n",
    "            second_half = sequence[i].split(forward_primer)[1]\n",
    "            second_half_list.append(second_half)\n",
    "        else:\n",
    "            out_count += 1\n",
    "            pass\n",
    "    print('\\n{} : {} sequence\\n'.format(forward_primer, len(second_half_list)))\n",
    "    # pd.DataFrame(second_half_list).to_csv(path + 'second_data/' + str(forward_primer) + '.csv', header=None,\n",
    "    #                                         index=None)\n",
    "    return pd.DataFrame(second_half_list).values.ravel()\n",
    "\n",
    "\n",
    "def exist_primer_check(path, forward_primer, sequence):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v5_Dataset/'\n",
    "\n",
    "    primers = pd.read_csv(path + 'forward_primer.csv', header=None).values.ravel()\n",
    "    count = 0\n",
    "    print('Start the the exist primer check...        {} in total\\n'.format(len(primers)))\n",
    "\n",
    "    # for forward_primer in primers:\n",
    "        # count += 1\n",
    "    reverse_primers = {}\n",
    "    print('The processing forward primer is    {}'.format(forward_primer))\n",
    "\n",
    "    # sequence = pd.read_csv(path + 'second_data/' + forward_primer + '.csv', header=None).values.ravel()\n",
    "    min_num = int(len(sequence) * 0.99)\n",
    "    sequence = pd.DataFrame(sequence)\n",
    "    p = 0\n",
    "\n",
    "    for primer in primers:\n",
    "        if forward_primer == primer:\n",
    "            pass\n",
    "        else:\n",
    "            count_feature = sequence[0].apply(lambda x: x.count(primer))\n",
    "            available_count = len(count_feature[count_feature == 1])\n",
    "            if available_count >= min_num:\n",
    "                p = 1\n",
    "                reverse_primers[primer] = available_count / len(sequence)\n",
    "\n",
    "    if p == 1:\n",
    "        print('Yes')\n",
    "        reverse_primers = pd.Series(reverse_primers).to_frame()\n",
    "        reverse_primers.to_csv(path + 'exist_primer_check/' + forward_primer + '.csv')\n",
    "    else:\n",
    "        print('No')\n",
    "    print('\\nComplete the exist primer check !\\n')\n",
    "\n",
    "\n",
    "def generate_random_sequence(sequence, number):\n",
    "    ''' Get the percentage of ATCG base pair in the sequence '''\n",
    "    vectorSize = 0\n",
    "    for i in range(len(sequence)):\n",
    "        if len(sequence[i]) > vectorSize:\n",
    "            vectorSize = len(sequence[i])\n",
    "\n",
    "    percent_A, percent_T, percent_C, percent_G = [], [], [], []\n",
    "    for seq in sequence:\n",
    "        counter = Counter(seq)\n",
    "\n",
    "        a = counter['A']\n",
    "        t = counter['T']\n",
    "        c = counter['C']\n",
    "        g = counter['G']\n",
    "\n",
    "        p_a = a / (a + t + c + g)\n",
    "        p_t = t / (a + t + c + g)\n",
    "        p_c = c / (a + t + c + g)\n",
    "        p_g = g / (a + t + c + g)\n",
    "\n",
    "        percent_A.append(p_a)\n",
    "        percent_T.append(p_t)\n",
    "        percent_C.append(p_c)\n",
    "        percent_G.append(p_g)\n",
    "\n",
    "    ''' generate a new random sequence '''\n",
    "    ATCG_random_list = []\n",
    "    ATCG_random_list = ATCG_random_list + ['A'] * round(vectorSize * int(np.mean(percent_A)))\n",
    "    ATCG_random_list = ATCG_random_list + ['T'] * round(vectorSize * int(np.mean(percent_T)))\n",
    "    ATCG_random_list = ATCG_random_list + ['C'] * round(vectorSize * int(np.mean(percent_C)))\n",
    "    ATCG_random_list = ATCG_random_list + ['G'] * round(vectorSize * int(np.mean(percent_G)))\n",
    "\n",
    "    print('\\nAverage \"A\" in the sequence      A --    {}'.format(np.mean(percent_A)))\n",
    "    print('Average \"T\" in the sequence      T --    {}'.format(np.mean(percent_T)))\n",
    "    print('Average \"C\" in the sequence      C --    {}'.format(np.mean(percent_C)))\n",
    "    print('Average \"G\" in the sequence      G --    {}\\n'.format(np.mean(percent_G)))\n",
    "\n",
    "    np.random.shuffle(ATCG_random_list)\n",
    "    if len(ATCG_random_list) != vectorSize:\n",
    "        if len(ATCG_random_list) > vectorSize:\n",
    "            n = len(ATCG_random_list) - vectorSize\n",
    "            ATCG_random_list = ATCG_random_list[:len(ATCG_random_list) - n]\n",
    "        elif len(ATCG_random_list) < vectorSize:\n",
    "            n = vectorSize - len(ATCG_random_list)\n",
    "            ATCG_random_list = ATCG_random_list + ['N'] * n\n",
    "\n",
    "    random_data_T = []\n",
    "    for i in range(number):\n",
    "        np.random.shuffle(ATCG_random_list)\n",
    "        var = ''.join(ATCG_random_list)\n",
    "        random_data_T.append(var)\n",
    "\n",
    "    random_data_V = []\n",
    "    for i in range(number):\n",
    "        np.random.shuffle(ATCG_random_list)\n",
    "        var = ''.join(ATCG_random_list)\n",
    "        random_data_V.append(var)\n",
    "\n",
    "    return random_data_T, random_data_V\n",
    "\n",
    "\n",
    "def reverse_sameFeature(path):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v5_Dataset/'\n",
    "\n",
    "    feature_files = glob.glob(path + 'feature/*')\n",
    "\n",
    "    p_value = 0\n",
    "    for file in feature_files:\n",
    "        if p_value == 0:\n",
    "            feature_data = list(pd.read_csv(file, header=None).values.ravel())\n",
    "            p_value = 1\n",
    "        elif p_value == 1:\n",
    "            feature_data_new = list(pd.read_csv(file, header=None).values.ravel())\n",
    "            for var in feature_data_new:\n",
    "                feature_data.append(var)\n",
    "\n",
    "    CountFeature_dict = Counter(feature_data)\n",
    "\n",
    "    RepeatList = []\n",
    "    nonRepeatList = list(set(feature_data))\n",
    "    for key in CountFeature_dict.keys():\n",
    "        value = CountFeature_dict.get(key)\n",
    "        if value != 1:\n",
    "            RepeatList.append(key)\n",
    "\n",
    "    pd.DataFrame(RepeatList).to_csv(path + 'Repeat_feature_List.csv', header=None, index=None)\n",
    "    pd.DataFrame(nonRepeatList).to_csv(path + 'nonRepeat_feature_List.csv', header=None, index=None)\n",
    "\n",
    "\n",
    "def reverse_CG_content_chect(path, min_CG=0.35, max_CG=0.65):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v5_Dataset/'\n",
    "    p_value = 0\n",
    "\n",
    "    feature_file = path + 'nonRepeat_feature_List.csv'\n",
    "    features = pd.read_csv(feature_file, header=None).values.ravel()\n",
    "\n",
    "    new_features = []\n",
    "    for feature in features:\n",
    "        if 'C' in feature and 'G' in feature:\n",
    "            CountFeature_dict = Counter(feature)\n",
    "            CG_content = CountFeature_dict.get('C') + CountFeature_dict.get('G')\n",
    "\n",
    "            if len(feature) * min_CG < CG_content < len(feature) * max_CG:\n",
    "                new_features.append(feature)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    Feature_List = list(set(new_features))\n",
    "    if len(Feature_List) == 0:\n",
    "        p_value = 2\n",
    "    else:\n",
    "        p_value = 1\n",
    "    pd.DataFrame(Feature_List).to_csv(path + 'nonRepeat_feature_List.csv', header=None, index=None)\n",
    "\n",
    "    return p_value\n",
    "\n",
    "\n",
    "def reverse_calculateAppearance(basic_path, path, variant_Name):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v5_Dataset/'\n",
    "\n",
    "    files = glob.glob(basic_path + 'Variant_virus/*')\n",
    "    p_value = 0\n",
    "\n",
    "    order = [3, 1, 0, 2, 4]  # original order = ['alpha', 'beta', 'gamma', 'delta', 'omicron']\n",
    "    for i in order:\n",
    "        file = files[i]\n",
    "        file = file.split('/')[-1]\n",
    "        save_fileName = file.split('.')[0].replace('GISAID_', '')\n",
    "        feature_file = path + 'nonRepeat_feature_List.csv'\n",
    "        seq_file = S3_to_sageMaker('seq', save_fileName)\n",
    "\n",
    "        print('Get the appearance in  -- {} --  virus'.format(save_fileName))\n",
    "        if variant_Name == 'omicron':\n",
    "            if save_fileName == 'omicron':\n",
    "                if p_value == 0:\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=0.80)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    if len(temp_feature) == 0:\n",
    "                        p_value = 2\n",
    "                    else:\n",
    "                        p_value = 1\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "\n",
    "                elif p_value == 1:\n",
    "                    feature_file = path + 'result/temp_feature.csv'\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=0.80)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    if len(temp_feature) == 0:\n",
    "                        p_value = 2\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "\n",
    "                elif p_value == 2:\n",
    "                    pass\n",
    "\n",
    "            elif save_fileName == 'delta':\n",
    "                if p_value == 0:\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=-1)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    if len(temp_feature) == 0:\n",
    "                        p_value = 2\n",
    "                    else:\n",
    "                        p_value = 1\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "\n",
    "                elif p_value == 1:\n",
    "                    feature_file = path + 'result/temp_feature.csv'\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=-1)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    if len(temp_feature) == 0:\n",
    "                        p_value = 2\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "\n",
    "                elif p_value == 2:\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                if p_value == 0:\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=0, accuracy=0.90)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    if len(temp_feature) == 0:\n",
    "                        p_value = 2\n",
    "                    else:\n",
    "                        p_value = 1\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "\n",
    "                elif p_value == 1:\n",
    "                    feature_file = path + 'result/temp_feature.csv'\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=0, accuracy=0.90)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    if len(temp_feature) == 0:\n",
    "                        p_value = 2\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "\n",
    "                elif p_value == 2:\n",
    "                    pass\n",
    "        else:\n",
    "            if save_fileName == 'omicron':\n",
    "                if p_value == 0:\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=-1)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    if len(temp_feature) == 0:\n",
    "                        p_value = 2\n",
    "                    else:\n",
    "                        p_value = 1\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "\n",
    "                elif p_value == 1:\n",
    "                    feature_file = path + 'result/temp_feature.csv'\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=-1)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    if len(temp_feature) == 0:\n",
    "                        p_value = 2\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "\n",
    "                elif p_value == 2:\n",
    "                    pass\n",
    "\n",
    "            elif save_fileName == variant_Name:\n",
    "                if p_value == 0:\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=0.95)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    if len(temp_feature) == 0:\n",
    "                        p_value = 2\n",
    "                    else:\n",
    "                        p_value = 1\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "\n",
    "                elif p_value == 1:\n",
    "                    feature_file = path + 'result/temp_feature.csv'\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=1, accuracy=0.95)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    if len(temp_feature) == 0:\n",
    "                        p_value = 2\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "\n",
    "                elif p_value == 2:\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                if p_value == 0:\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=0, accuracy=0.95)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    if len(temp_feature) == 0:\n",
    "                        p_value = 2\n",
    "                    else:\n",
    "                        p_value = 1\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "\n",
    "                elif p_value == 1:\n",
    "                    feature_file = path + 'result/temp_feature.csv'\n",
    "                    featureDF = get_appearance(feature_file, seq_file, high_type=0, accuracy=0.95)\n",
    "                    featureDF.to_csv(path + 'Seq_appearance/feature_' + save_fileName + '.csv')\n",
    "\n",
    "                    temp_file = path + 'Seq_appearance/feature_' + save_fileName + '.csv'\n",
    "                    temp_data = pd.read_csv(temp_file)\n",
    "                    temp_feature = temp_data['Unnamed: 0'].values.ravel()\n",
    "                    if len(temp_feature) == 0:\n",
    "                        p_value = 2\n",
    "                    pd.DataFrame(temp_feature).to_csv(path + 'result/temp_feature.csv', header=None, index=None)\n",
    "\n",
    "                elif p_value == 2:\n",
    "                    pass\n",
    "\n",
    "    return p_value\n",
    "\n",
    "\n",
    "def reverse_commonFeatureAppearance(path, forward_primer):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v5_Dataset/'\n",
    "\n",
    "    files = glob.glob(path + 'Seq_appearance/*')\n",
    "\n",
    "    Name_list = []\n",
    "    p_value = 0\n",
    "    for file in files:\n",
    "        file_name = file.split('/')[-1].split('.')[0]\n",
    "        Name_list.append(file_name)\n",
    "\n",
    "        if p_value == 0:\n",
    "            data = pd.read_csv(file, header=None)\n",
    "            data = data[0][data[0].notna()].values.ravel()\n",
    "            p_value = 1\n",
    "        elif p_value == 1:\n",
    "            data_new = pd.read_csv(file, header=None)\n",
    "            data_new = data_new[0][data_new[0].notna()].values.ravel()\n",
    "\n",
    "            data = [x for x in data if x in data_new]\n",
    "\n",
    "    Appearance_list = []\n",
    "    for seq in data:\n",
    "        print('\\nFor the sequence: {}\\n'.format(seq))\n",
    "        temp_list = []\n",
    "        for file in files:\n",
    "            file_name = file.split('/')[-1].split('.')[0]\n",
    "            data_appearance = pd.read_csv(file, header=None)\n",
    "            appearance = data_appearance[data_appearance[0] == seq][1].values.ravel()[0]\n",
    "            # print('{} :  {}'.format(file_name, appearance))\n",
    "            temp_list.append(appearance)\n",
    "        Appearance_list.append(temp_list)\n",
    "\n",
    "    Appearance_DF = pd.DataFrame(Appearance_list)\n",
    "    Appearance_DF.index = data\n",
    "    Appearance_DF.columns = Name_list\n",
    "\n",
    "    Appearance_DF = Appearance_DF[\n",
    "        ['feature_alpha', 'feature_beta', 'feature_gamma', 'feature_delta', 'feature_omicron']]\n",
    "\n",
    "    Appearance_DF = Appearance_DF.sort_values(by=['feature_omicron'], ascending=[False])\n",
    "    Appearance_DF.to_csv(path + 'result/' + forward_primer + '_Appearance_DataFrame.csv')\n",
    "\n",
    "\n",
    "def get_available_primers(path):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v4_Dataset/'\n",
    "    print('Get available primers with 40%-60% CG content...')\n",
    "    if os.path.exists(path + 'result/temp_feature.csv'):\n",
    "        os.remove(path + 'result/temp_feature.csv')\n",
    "\n",
    "    features = pd.read_csv(path + 'forward_primer_CG_check.csv', header=None).values.ravel()\n",
    "\n",
    "    files = glob.glob(path + 'result/*')\n",
    "    for file in files:\n",
    "        forward_primer = file.split('/')[-1].split('_')[0]\n",
    "        if forward_primer in features:\n",
    "            newpath = path + 'CG_Check/new_primers/' + forward_primer + '.csv'\n",
    "            shutil.copyfile(file, newpath)\n",
    "\n",
    "    files = glob.glob(path + 'exist_primer_check/*')\n",
    "    for file in files:\n",
    "        forward_primer = file.split('/')[-1].split('.')[0]\n",
    "        if forward_primer in features:\n",
    "            newpath = path + 'CG_Check/exist_primers/' + forward_primer + '.csv'\n",
    "            shutil.copyfile(file, newpath)\n",
    "\n",
    "    print('Finished ! \\n')\n",
    "\n",
    "\n",
    "def calculate_CG_content_and_melting_temperature(primers):\n",
    "    primers = list(primers)\n",
    "    if len(set(primers)) == 4:\n",
    "        counter = Counter(primers)\n",
    "        Tm = 64.9 + 41 * (counter.get('G') + counter.get('C') - 16.4) / (counter.get('A') + counter.get('T') +\n",
    "                                                                         counter.get('G') + counter.get('C'))\n",
    "        CG_content = (counter.get('C') + counter.get('G')) / len(primers)\n",
    "        return CG_content, Tm\n",
    "    else:\n",
    "        return -1, -1\n",
    "\n",
    "\n",
    "def length_of_amplicon(path, sequence_full):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v4_Dataset/'\n",
    "\n",
    "    print('Get the length of amplicon between the primers...\\n')\n",
    "    feature_CG_Check = pd.read_csv(path + 'forward_primer_CG_check.csv', header=None).values.ravel()\n",
    "    primer_design = []\n",
    "\n",
    "    '''--------------------------------------------------------------------------------------------------------------'''\n",
    "    print('(1/2) Exist primers: ')\n",
    "    files = glob.glob(path + 'CG_Check/exist_primers/*')\n",
    "    count = 0\n",
    "\n",
    "    for file in files:\n",
    "        count += 1\n",
    "        forward_primer = file.split('/')[-1].split('.')[0]\n",
    "        print('No.{} out of {} file:          {}'.format(count, len(files), forward_primer))\n",
    "\n",
    "        sequence = get_after_primer_data(path, forward_primer, sequence_full)\n",
    "        reverse_primers = pd.read_csv(file)['Unnamed: 0'].values.ravel()\n",
    "        for reverse_primer in reverse_primers:\n",
    "            length = []\n",
    "            if reverse_primer in feature_CG_Check:\n",
    "                for seq in sequence:\n",
    "                    if reverse_primer in seq:\n",
    "                        amplicon = len(seq.split(reverse_primer)[0]) + len(forward_primer) + len(reverse_primer)\n",
    "                        length.append(amplicon)\n",
    "                    else:\n",
    "                        pass\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if len(length) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                maxx = np.max(length)\n",
    "                minn = np.min(length)\n",
    "                mean = np.mean(length)\n",
    "                f_CG, f_Tm = calculate_CG_content_and_melting_temperature(forward_primer)\n",
    "                r_CG, r_Tm = calculate_CG_content_and_melting_temperature(reverse_primer)\n",
    "\n",
    "                primer_design.append(forward_primer)\n",
    "                primer_design.append(reverse_primer)\n",
    "                primer_design.append(f_CG)\n",
    "                primer_design.append(r_CG)\n",
    "                primer_design.append(f_Tm)\n",
    "                primer_design.append(r_Tm)\n",
    "                primer_design.append(abs(f_Tm - r_Tm))\n",
    "\n",
    "                primer_design.append(mean)\n",
    "                primer_design.append(maxx)\n",
    "                primer_design.append(minn)\n",
    "\n",
    "    pd.DataFrame(primer_design).to_csv(path + 'amplicon_length/exist_primers.csv', header=None, index=None)\n",
    "\n",
    "    '''--------------------------------------------------------------------------------------------------------------'''\n",
    "    print('\\n(2/2) new primers: ')\n",
    "    primer_design = []\n",
    "    files = glob.glob(path + 'CG_Check/new_primers/*')\n",
    "    count = 0\n",
    "\n",
    "    for file in files:\n",
    "        count += 1\n",
    "        forward_primer = file.split('/')[-1].split('.')[0]\n",
    "        print('No.{} out of {} file:          {}'.format(count, len(files), forward_primer))\n",
    "\n",
    "        sequence = get_after_primer_data(path, forward_primer, sequence_full)\n",
    "        reverse_primers = pd.read_csv(file)['Unnamed: 0'].values.ravel()\n",
    "        for reverse_primer in reverse_primers:\n",
    "            length = []\n",
    "            for seq in sequence:\n",
    "                if reverse_primer in seq:\n",
    "                    amplicon = len(seq.split(reverse_primer)[0]) + len(forward_primer) + len(reverse_primer)\n",
    "                    length.append(amplicon)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            if len(length) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                maxx = np.max(length)\n",
    "                minn = np.min(length)\n",
    "                mean = np.mean(length)\n",
    "                f_CG, f_Tm = calculate_CG_content_and_melting_temperature(forward_primer)\n",
    "                r_CG, r_Tm = calculate_CG_content_and_melting_temperature(reverse_primer)\n",
    "\n",
    "                primer_design.append(forward_primer)\n",
    "                primer_design.append(reverse_primer)\n",
    "                primer_design.append(f_CG)\n",
    "                primer_design.append(r_CG)\n",
    "                primer_design.append(f_Tm)\n",
    "                primer_design.append(r_Tm)\n",
    "                primer_design.append(abs(f_Tm - r_Tm))\n",
    "\n",
    "                primer_design.append(mean)\n",
    "                primer_design.append(maxx)\n",
    "                primer_design.append(minn)\n",
    "\n",
    "    pd.DataFrame(primer_design).to_csv(path + 'amplicon_length/new_primers.csv', header=None, index=None)\n",
    "\n",
    "\n",
    "def reshape_file(path):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v4_Dataset/'\n",
    "\n",
    "    Name_list = ['Forward primer', 'Reverse primer', 'Forward CG content', 'Reverse CG content',\n",
    "                 'Forward Melting Temperature (Tm)', 'Reverse Melting Temperature (Tm)', 'Tm difference',\n",
    "                 'amplicon_avg', 'amplicon_max', 'amplicon_min']\n",
    "\n",
    "    new_primers = pd.read_csv(path + 'amplicon_length/new_primers.csv', header=None).values.ravel()\n",
    "    new_primers = new_primers.reshape(-1, len(Name_list))\n",
    "    new_primers = pd.DataFrame(new_primers)\n",
    "    new_primers.columns = Name_list\n",
    "    new_primers.to_csv(path + 'amplicon_length/new_primers.csv', index=None)\n",
    "\n",
    "    exist_primers = pd.read_csv(path + 'amplicon_length/exist_primers.csv', header=None).values.ravel()\n",
    "    exist_primers = exist_primers.reshape(-1, len(Name_list))\n",
    "    exist_primers = pd.DataFrame(exist_primers)\n",
    "    exist_primers.columns = Name_list\n",
    "    exist_primers.to_csv(path + 'amplicon_length/exist_primers.csv', index=None)\n",
    "\n",
    "\n",
    "def reverse(path):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v5_Dataset/'\n",
    "    new_reverse_primer = []\n",
    "    new_Reverse_Melting_Temperature = []\n",
    "\n",
    "    '''--------------------------------------------------------------------------------------------------------------'''\n",
    "    print('(1/2) Exist primers: ')\n",
    "    Final_primers = pd.read_csv(path + 'amplicon_length/exist_primers.csv')\n",
    "    reverse_primer = Final_primers['Reverse primer'].values.ravel()\n",
    "    for i in range(len(reverse_primer)):\n",
    "        temp = list(reverse_primer[i])\n",
    "        new_primer = []\n",
    "        for j in range(len(temp)):\n",
    "            if temp[len(temp) - j - 1] == 'A':\n",
    "                new_primer.append('T')\n",
    "            elif temp[len(temp) - j - 1] == 'T':\n",
    "                new_primer.append('A')\n",
    "            elif temp[len(temp) - j - 1] == 'G':\n",
    "                new_primer.append('C')\n",
    "            elif temp[len(temp) - j - 1] == 'C':\n",
    "                new_primer.append('G')\n",
    "\n",
    "        if len(set(new_primer)) == 4:\n",
    "            counter = Counter(new_primer)\n",
    "            Tm = 64.9 + 41 * (counter.get('G') + counter.get('C') - 16.4) / (counter.get('A') + counter.get('T') +\n",
    "                                                                             counter.get('G') + counter.get('C'))\n",
    "        else:\n",
    "            Tm = -1\n",
    "\n",
    "        var = ''.join(new_primer)\n",
    "        new_reverse_primer.append(var)\n",
    "        new_Reverse_Melting_Temperature.append(Tm)\n",
    "\n",
    "    Final_primers['Reverse primer'] = new_reverse_primer\n",
    "    Final_primers['Reverse Melting Temperature (Tm)'] = new_Reverse_Melting_Temperature\n",
    "    Final_primers['Tm difference'] = abs(Final_primers['Forward Melting Temperature (Tm)'] -\n",
    "                                         Final_primers['Reverse Melting Temperature (Tm)'])\n",
    "    Final_primers.to_csv(path + 'amplicon_length/r_exist_primers.csv', index=None)\n",
    "    print('Finished !')\n",
    "\n",
    "    '''--------------------------------------------------------------------------------------------------------------'''\n",
    "    new_reverse_primer = []\n",
    "    new_Reverse_Melting_Temperature = []\n",
    "\n",
    "    print('\\n(2/2) new primers')\n",
    "    Final_primers = pd.read_csv(path + 'amplicon_length/new_primers.csv')\n",
    "    reverse_primer = Final_primers['Reverse primer'].values.ravel()\n",
    "    for i in range(len(reverse_primer)):\n",
    "        temp = list(reverse_primer[i])\n",
    "        new_primer = []\n",
    "        for j in range(len(temp)):\n",
    "            if temp[len(temp) - j - 1] == 'A':\n",
    "                new_primer.append('T')\n",
    "            elif temp[len(temp) - j - 1] == 'T':\n",
    "                new_primer.append('A')\n",
    "            elif temp[len(temp) - j - 1] == 'G':\n",
    "                new_primer.append('C')\n",
    "            elif temp[len(temp) - j - 1] == 'C':\n",
    "                new_primer.append('G')\n",
    "\n",
    "        if len(set(new_primer)) == 4:\n",
    "            counter = Counter(new_primer)\n",
    "            Tm = 64.9 + 41 * (counter.get('G') + counter.get('C') - 16.4) / (counter.get('A') + counter.get('T') +\n",
    "                                                                             counter.get('G') + counter.get('C'))\n",
    "        else:\n",
    "            Tm = -1\n",
    "\n",
    "        var = ''.join(new_primer)\n",
    "        new_reverse_primer.append(var)\n",
    "        new_Reverse_Melting_Temperature.append(Tm)\n",
    "\n",
    "    Final_primers['Reverse primer'] = new_reverse_primer\n",
    "    Final_primers['Reverse Melting Temperature (Tm)'] = new_Reverse_Melting_Temperature\n",
    "    Final_primers['Tm difference'] = abs(Final_primers['Forward Melting Temperature (Tm)'] -\n",
    "                                         Final_primers['Reverse Melting Temperature (Tm)'])\n",
    "    Final_primers.to_csv(path + 'amplicon_length/r_new_primers.csv', index=None)\n",
    "    print('Finished !')\n",
    "\n",
    "\n",
    "def primer_design_rules(path):\n",
    "    # path = '/Users/harry/Documents/Sars-Cov-2 Project/v5_Dataset/'\n",
    "\n",
    "    '''--------------------------------------------------------------------------------------------------------------'''\n",
    "    print('(1/2) Exist primers: ')\n",
    "    primers = pd.read_csv(path + 'amplicon_length/r_exist_primers.csv')\n",
    "    drop_list = []\n",
    "\n",
    "    for i in range(len(primers)):\n",
    "        temp = primers[i:i + 1]\n",
    "        forward_primer = temp['Forward primer'][i]\n",
    "        reverse_primer = temp['Reverse primer'][i]\n",
    "\n",
    "        if 'C' not in forward_primer[-3:] and 'G' not in forward_primer[-3:]:\n",
    "            drop_list.append(i)\n",
    "        else:\n",
    "            if 'C' not in reverse_primer[-3:] and 'G' not in reverse_primer[-3:]:\n",
    "                drop_list.append(i)\n",
    "            else:\n",
    "                if primer3.calcHomodimer(forward_primer).dg <= -9000 or primer3.calcHomodimer(\n",
    "                        reverse_primer).dg <= -9000:\n",
    "                    drop_list.append(i)\n",
    "                else:\n",
    "                    if primer3.calcHeterodimer(forward_primer, reverse_primer).dg <= -9000:\n",
    "                        drop_list.append(i)\n",
    "                    else:\n",
    "                        if abs(primer3.calcTm(forward_primer) - primer3.calcTm(reverse_primer)) > 5:\n",
    "                            drop_list.append(i)\n",
    "                        else:\n",
    "                            if primer3.calcTm(forward_primer) > 60 or primer3.calcTm(reverse_primer) < 50:\n",
    "                                drop_list.append(i)\n",
    "                            else:\n",
    "                                if primer3.calcTm(reverse_primer) > 60 or primer3.calcTm(reverse_primer) < 50:\n",
    "                                    drop_list.append(i)\n",
    "                                else:\n",
    "                                    primers.iloc[i:i + 1, 4] = primer3.calcTm(forward_primer)\n",
    "                                    primers.iloc[i:i + 1, 5] = primer3.calcTm(reverse_primer)\n",
    "                                    primers.iloc[i:i + 1, 6] = abs(\n",
    "                                        primer3.calcTm(forward_primer) - primer3.calcTm(reverse_primer))\n",
    "\n",
    "    primers = primers.drop(index=drop_list)\n",
    "    primers.to_csv(path + 'amplicon_length/r_exist_primers_deltaG.csv', index=None)\n",
    "    print('Finished !')\n",
    "\n",
    "    '''--------------------------------------------------------------------------------------------------------------'''\n",
    "    print('\\n(2/2) new primers')\n",
    "\n",
    "    primers = pd.read_csv(path + 'amplicon_length/r_new_primers.csv')\n",
    "    drop_list = []\n",
    "\n",
    "    for i in range(len(primers)):\n",
    "        temp = primers[i:i + 1]\n",
    "        forward_primer = temp['Forward primer'][i]\n",
    "        reverse_primer = temp['Reverse primer'][i]\n",
    "\n",
    "        if 'C' not in forward_primer[-3:] and 'G' not in forward_primer[-3:]:\n",
    "            drop_list.append(i)\n",
    "        else:\n",
    "            if 'C' not in reverse_primer[-3:] and 'G' not in reverse_primer[-3:]:\n",
    "                drop_list.append(i)\n",
    "            else:\n",
    "                if primer3.calcHomodimer(forward_primer).dg <= -9000 or primer3.calcHomodimer(\n",
    "                        reverse_primer).dg <= -9000:\n",
    "                    drop_list.append(i)\n",
    "                else:\n",
    "                    if primer3.calcHeterodimer(forward_primer, reverse_primer).dg <= -9000:\n",
    "                        drop_list.append(i)\n",
    "                    else:\n",
    "                        if abs(primer3.calcTm(forward_primer) - primer3.calcTm(reverse_primer)) > 5:\n",
    "                            drop_list.append(i)\n",
    "                        else:\n",
    "                            if primer3.calcTm(forward_primer) > 60 or primer3.calcTm(reverse_primer) < 50:\n",
    "                                drop_list.append(i)\n",
    "                            else:\n",
    "                                if primer3.calcTm(reverse_primer) > 60 or primer3.calcTm(reverse_primer) < 50:\n",
    "                                    drop_list.append(i)\n",
    "                                else:\n",
    "                                    primers.iloc[i:i + 1, 4] = primer3.calcTm(forward_primer)\n",
    "                                    primers.iloc[i:i + 1, 5] = primer3.calcTm(reverse_primer)\n",
    "                                    primers.iloc[i:i + 1, 6] = abs(\n",
    "                                        primer3.calcTm(forward_primer) - primer3.calcTm(reverse_primer))\n",
    "\n",
    "    primers = primers.drop(index=drop_list)\n",
    "    primers.to_csv(path + 'amplicon_length/r_new_primers_deltaG.csv', index=None)\n",
    "    print('Finished !')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taF6-kk6UoaB"
   },
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wf3CcpE9WMLK"
   },
   "outputs": [],
   "source": [
    "forward_batchSize = 50\n",
    "reverse_batchSize = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oicj5WllVCTU"
   },
   "outputs": [],
   "source": [
    "basic_path = './'\n",
    "sys_path(basic_path)\n",
    "\n",
    "variant_list = ['alpha', 'beta', 'gamma', 'delta', 'omicron']  # Can change the order\n",
    "\n",
    "primer_length = 21\n",
    "\n",
    "generate_forward_number = 2000\n",
    "train_model_number_forward = 2000\n",
    "\n",
    "generate_reverse_number = 2000\n",
    "train_model_number_reverse = 20000\n",
    "# get_data(basic_path) # Run only once\n",
    "vectorSize = mixed_data(basic_path, delta_num=train_model_number_forward, other_percent=1)\n",
    "\n",
    "'-------------------------------------------------------------------------------------------------------------'\n",
    "'----------------------------------------  Forward Method  ---------------------------------------------------'\n",
    "'-------------------------------------------------------------------------------------------------------------'\n",
    "\n",
    "forward_method = 'Pooling'  # ['Pooling', 'Top', 'Combination']\n",
    "forward_max_Pooling_window_size = 148\n",
    "'-----------------------------------------------------------------'\n",
    "# forward_method = 'Top'\n",
    "forward_top_number = 175\n",
    "'-----------------------------------------------------------------'\n",
    "# forward_method = 'Combination'\n",
    "# forward_max_Pooling_window_size = 500\n",
    "forward_top_in_window = 10\n",
    "\n",
    "'-------------------------------------------------------------------------------------------------------------'\n",
    "'----------------------------------------  Reverse Method  ---------------------------------------------------'\n",
    "'-------------------------------------------------------------------------------------------------------------'\n",
    "\n",
    "# reverse_method = 'Pooling'  # ['Pooling', 'Top', 'Combination']\n",
    "# reverse_max_Pooling_window_size = 148\n",
    "'-----------------------------------------------------------------'\n",
    "reverse_method = 'Top'\n",
    "reverse_top_number = 300\n",
    "'-----------------------------------------------------------------'\n",
    "# reverse_method = 'Combination'\n",
    "# reverse_max_Pooling_window_size = 500\n",
    "reverse_top_in_window = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YAZ4yH0tVjNV"
   },
   "outputs": [],
   "source": [
    "train_valid_data(basic_path, n_splits=2)\n",
    "if forward_method == 'Top':\n",
    "    forward_max_Pooling_window_size = int(vectorSize / forward_top_number) + 1\n",
    "\n",
    "seq_T = pd.read_csv(basic_path + 'index/train_sequence.csv', header=None).values.ravel()\n",
    "label_T = pd.read_csv(basic_path + 'index/train_label.csv', header=None).values.ravel()\n",
    "\n",
    "seq_V = pd.read_csv(basic_path + 'index/valid_sequence.csv', header=None).values.ravel()\n",
    "label_V = pd.read_csv(basic_path + 'index/valid_label.csv', header=None).values.ravel()\n",
    "\n",
    "seq_Test = pd.read_csv(basic_path + 'index/train_sequence.csv', header=None).values.ravel()\n",
    "label_Test = pd.read_csv(basic_path + 'index/train_label.csv', header=None).values.ravel()\n",
    "\n",
    "rand = np.random.randint(100000)\n",
    "np.random.seed(rand)\n",
    "np.random.shuffle(seq_T)\n",
    "np.random.seed(rand)\n",
    "np.random.shuffle(label_T)\n",
    "\n",
    "rand = np.random.randint(100000)\n",
    "np.random.seed(rand)\n",
    "np.random.shuffle(seq_V)\n",
    "np.random.seed(rand)\n",
    "np.random.shuffle(label_V)\n",
    "\n",
    "rand = np.random.randint(100000)\n",
    "np.random.seed(rand)\n",
    "np.random.shuffle(seq_Test)\n",
    "np.random.seed(rand)\n",
    "np.random.shuffle(label_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FL0bDM2tV1zn"
   },
   "source": [
    "## forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ril2IvgmV4sy"
   },
   "outputs": [],
   "source": [
    "f = open(basic_path + 'model/outputVector.txt', 'w')\n",
    "f.write('1\\n')\n",
    "f.write('1\\n')\n",
    "\n",
    "# Parameters\n",
    "batchSize = forward_batchSize\n",
    "labelSize = int(np.max(label_T) + 1)\n",
    "limit = 1.01\n",
    "iterMax = 50\n",
    "# ------------------------------------------------\n",
    "w1 = 12\n",
    "wd1 = 21\n",
    "h1 = forward_max_Pooling_window_size  # 31029/148 ~ 210 ---> 31079/148 ~ 210\n",
    "w4 = 256\n",
    "# ------------------------------------------------\n",
    "# initialize variables\n",
    "iter = 0\n",
    "train_accuracy = 0.0\n",
    "valid_accuracy = 0.0\n",
    "test_accuracy = 0.0\n",
    "# best validation accuracy\n",
    "best = 0\n",
    "validWindow = [0, 0, 0]\n",
    "repeatWindow = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "validBest = 1e6\n",
    "yResult = []\n",
    "yTest = []\n",
    "\n",
    "oneHot_labels_T = oneHot(label_T, labelSize)\n",
    "oneHot_labels_V = oneHot(label_V, labelSize)\n",
    "oneHot_labels_Test = oneHot(label_Test, labelSize)\n",
    "runs = int(len(oneHot_labels_T) / batchSize)\n",
    "\n",
    "'----------------------------------------------------------------------------------------------------------'\n",
    "\n",
    "# Tensorflow CNN model\n",
    "print('\\nStrat to build the CNN model...')\n",
    "tf.disable_v2_behavior()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, vectorSize])  # input variable\n",
    "keep_prob = tf.placeholder(tf.float32)  # keep between 0.50 to 1.0\n",
    "y_ = tf.placeholder(tf.float32, [None, labelSize])  # expected outputs variable\n",
    "x_image0 = tf.reshape(x, [-1, 1, vectorSize, 1])  # arrange the tensor as an image (1*30145) 1 channel\n",
    "x_image = tf.transpose(x_image0, perm=[0, 3, 2, 1])  # arrange the tensor into 1 channels (1*30145)\n",
    "\n",
    "# 1 LAYER\n",
    "W_conv1 = weight_variable([1, wd1, 1, w1])\n",
    "b_conv1 = bias_variable([w1])\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 1, h1, 1], strides=[1, 1, h1, 1], padding='SAME')\n",
    "\n",
    "# Rectifier LAYER\n",
    "coef = int(h_pool1.get_shape()[1] * h_pool1.get_shape()[2] * h_pool1.get_shape()[3])  # 1 * 209.34 * 12 ~ 2512\n",
    "h_pool2_flat = tf.reshape(h_pool1, [-1, coef])\n",
    "W_fc1 = weight_variable([coef, w4])\n",
    "b_fc1 = bias_variable([w4])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Rectifier-Dropout LAYER\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "W_fc2 = weight_variable([w4, labelSize])\n",
    "b_fc2 = bias_variable([labelSize])\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "# Loss Function\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_conv, labels=y_) + 0.001 * tf.nn.l2_loss(W_conv1))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "trueResult = tf.argmax(y_conv, 1)\n",
    "trueTest = tf.argmax(y_, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('\\nFinished the model building!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luETtuAqWjdB"
   },
   "source": [
    "### new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mR6n6NwMWVqr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())\n",
    "saver = tf.train.Saver()\n",
    "plt.ion()\n",
    "\n",
    "while (best < limit) & (iter < iterMax):\n",
    "    indexBatch = []\n",
    "    for iB in range(0, len(oneHot_labels_T)):\n",
    "        indexBatch.append(iB)\n",
    "        random.shuffle(indexBatch)\n",
    "    for run in range(0, runs):\n",
    "        xa, ya = getBatch_run(seq_T, oneHot_labels_T, batchSize, run, indexBatch, vectorSize)\n",
    "        train_step.run(feed_dict={x: xa, y_: ya, keep_prob: 0.5})\n",
    "\n",
    "    xa, ya = getBatch(seq_T, oneHot_labels_T, batchSize, vectorSize)\n",
    "    train_accuracy = accuracy.eval(feed_dict={x: xa, y_: ya, keep_prob: 1.0})\n",
    "\n",
    "    xaV, yaV = getBatch(seq_V, oneHot_labels_V, oneHot_labels_V.shape[0], vectorSize)\n",
    "    valid_accuracy = accuracy.eval(feed_dict={x: xaV, y_: yaV, keep_prob: 1.0})\n",
    "    cross_entropyVal = cross_entropy.eval(feed_dict={x: xaV, y_: yaV, keep_prob: 1.0})\n",
    "    cross_entropyTrain = cross_entropy.eval(feed_dict={x: xa, y_: ya, keep_prob: 1.0})\n",
    "\n",
    "    validWindowValue = 0\n",
    "    tempValid = validWindow\n",
    "    for i in range(0, len(validWindow) - 1):\n",
    "        tempValid[i] = validWindow[i + 1]\n",
    "    for i in range(0, len(validWindow)):\n",
    "        validWindow[i] = tempValid[i]\n",
    "    validWindow[len(validWindow) - 1] = valid_accuracy\n",
    "    for i in range(0, len(validWindow)):\n",
    "        validWindowValue = validWindowValue + validWindow[i]\n",
    "    validWindowValue = validWindowValue / len(validWindow)\n",
    "    tempValid = repeatWindow\n",
    "    for i in range(0, len(repeatWindow) - 1):\n",
    "        tempValid[i] = repeatWindow[i + 1]\n",
    "    for i in range(0, len(repeatWindow)):\n",
    "        repeatWindow[i] = tempValid[i]\n",
    "    repeatWindow[len(repeatWindow) - 1] = valid_accuracy\n",
    "    if np.var(repeatWindow) == 0 and iter > iterMax:\n",
    "        iter = iter\n",
    "    if validWindowValue > best or cross_entropyVal < validBest:\n",
    "        validBest = cross_entropyVal\n",
    "        best = validWindowValue\n",
    "\n",
    "        xaT, yaT = getBatch(seq_V, oneHot_labels_V, oneHot_labels_V.shape[0], vectorSize)\n",
    "        test_accuracy = accuracy.eval(feed_dict={x: xaT, y_: yaT, keep_prob: 1.0})\n",
    "        save_path = saver.save(sess, basic_path + \"model/CNN_model.ckpt\")\n",
    "\n",
    "        results = correct_prediction.eval(feed_dict={x: xaT, y_: yaT, keep_prob: 1.0})\n",
    "        yResult = trueResult.eval(feed_dict={x: xaT, y_: yaT, keep_prob: 1.0})\n",
    "        yTest = trueTest.eval(feed_dict={x: xaT, y_: yaT, keep_prob: 1.0})\n",
    "\n",
    "        fOut = open(basic_path + 'model/outputVector_1.txt', 'w')\n",
    "        fOut.write('1\\n')\n",
    "        fOut.write('1\\n')\n",
    "        temp = 1.0 - best\n",
    "        trueAcc = str(temp)\n",
    "        print(trueAcc)\n",
    "        fOut.write(trueAcc + '\\n')\n",
    "        fOut.close()\n",
    "    log = \"%d\t%g\t%g\t%g\t%g\t%g\t%g\" % (iter, train_accuracy, valid_accuracy, best,\n",
    "                                                      test_accuracy, cross_entropyVal, cross_entropyTrain)\n",
    "    print(log)\n",
    "    f.write(log + '\\n')\n",
    "    iter = iter + 1\n",
    "\n",
    "plt.ioff()\n",
    "\n",
    "f.close()\n",
    "np.savetxt(basic_path + 'model/results.txt', yResult, fmt='%i', delimiter=' ')\n",
    "np.savetxt(basic_path + 'model/test.txt', yTest, fmt='%i', delimiter=' ')\n",
    "f = open(basic_path + 'model/log3.txt', 'a')\n",
    "name = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' / CNN model'\n",
    "f.write(name + '\\n')\n",
    "f.close()\n",
    "\n",
    "f = open(basic_path + 'model/outputVector_2.txt', 'w')\n",
    "f.write('1\\n')\n",
    "f.write('1\\n')\n",
    "temp = 1.0 - best\n",
    "trueAcc = str(temp)\n",
    "print(trueAcc)\n",
    "f.write(trueAcc + '\\n')\n",
    "f.close()\n",
    "\n",
    "xaT, yaT = getBatch(seq_V, oneHot_labels_V, oneHot_labels_V.shape[0], vectorSize)\n",
    "\n",
    "units = sess.run(h_conv1, feed_dict={x: xaT, y_: yaT, keep_prob: 1.0})\n",
    "print(units.shape)\n",
    "units = sess.run(h_pool1, feed_dict={x: xaT, y_: yaT, keep_prob: 1.0})\n",
    "print(units.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd2mAM_xWlhA"
   },
   "source": [
    "### exsit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QG4gsZrWpy6"
   },
   "outputs": [],
   "source": [
    "# sess.run(tf.initialize_all_variables())\n",
    "# saver = tf.train.Saver()\n",
    "# saver.restore(sess, basic_path + 'model/CNN_model.ckpt')\n",
    "\n",
    "# print('\\n\\nLoad CNN model complete!\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6qv5DBRXj_A"
   },
   "source": [
    "### forward primer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRiCT37lXi33"
   },
   "outputs": [],
   "source": [
    "for variant_Name in variant_list:\n",
    "    path = basic_path + variant_Name + '/forward/'\n",
    "    seqName_file = S3_to_sageMaker('seqName', variant_Name)\n",
    "    seqName = pd.read_csv(seqName_file, header=None).values.ravel()  # get the sequence name\n",
    "\n",
    "    seq_file = S3_to_sageMaker('seq', variant_Name)\n",
    "    sequence = pd.read_csv(seq_file, header=None).values.ravel()  # get the sequence\n",
    "\n",
    "    np.random.shuffle(seqName)\n",
    "    np.random.shuffle(sequence)\n",
    "    seqName = seqName[:generate_forward_number]\n",
    "    sequence = sequence[:generate_forward_number]\n",
    "    seq_labels = np.array([0 for x in range(len(seqName))])\n",
    "\n",
    "    seq_count = 0\n",
    "    outData, outLabels = [], []\n",
    "    print('\\nStrat to transfer the sequence ...\\n')\n",
    "    for i in range(len(sequence)):\n",
    "        seq_count += 1\n",
    "        if seq_count % 100 == 0:\n",
    "            print('Transferring...  No.{} with total {}'.format(seq_count, len(seq_labels)))\n",
    "        sample = np.zeros(vectorSize)\n",
    "        for j in range(0, len(sequence[i])):\n",
    "            if sequence[i][j] == 'C':\n",
    "                sample[j] = 0.25\n",
    "            elif sequence[i][j] == 'T':\n",
    "                sample[j] = 0.50\n",
    "            elif sequence[i][j] == 'G':\n",
    "                sample[j] = 0.75\n",
    "            elif sequence[i][j] == 'A':\n",
    "                sample[j] = 1.0\n",
    "            else:\n",
    "                sample[j] = 0.0\n",
    "        outData.append(sample)\n",
    "        outLabels.append(seq_labels[i])\n",
    "\n",
    "    print('\\nTransfer finished!\\n')\n",
    "    pd.DataFrame(sequence).to_csv(path + 'filter_seq/filter_seq.csv', header=None, index=None)\n",
    "    data = np.array(outData)\n",
    "    seq_labels = np.array(outLabels)\n",
    "    oneHotLabels = oneHot(seq_labels, labelSize)\n",
    "\n",
    "    '----------------------------------------------------------------------------------------------------------'\n",
    "    'Filter files'\n",
    "\n",
    "    print('the Filter files')\n",
    "    units = sess.run(h_conv1, feed_dict={x: data, y_: oneHotLabels, keep_prob: 1.0})\n",
    "    print('\\nThe first h_conv1 layer size:      h_conv1 = {}\\n'.format(units.shape))\n",
    "\n",
    "    sampleSize = int(data.shape[0])\n",
    "    Mat = np.zeros((sampleSize, vectorSize))\n",
    "\n",
    "    for filterIndex in range(units.shape[3]):\n",
    "        print('Loop 1 : Generating the  {}  Filter file'.format(filterIndex))\n",
    "        for testSize in range(sampleSize):\n",
    "            for inputSize in range(vectorSize):\n",
    "                Mat[testSize][inputSize] = units[testSize][0][inputSize][filterIndex]\n",
    "        pd.DataFrame(Mat).to_csv(path + 'filter/filter_' + str(filterIndex) + '.csv', header=None, index=None)\n",
    "\n",
    "\n",
    "    if forward_method == 'Pooling':\n",
    "        numberWindows = posPool(path, vectorSize, forward_max_Pooling_window_size)\n",
    "    elif forward_method == 'Top':\n",
    "        numberWindows = posPool_top(path, vectorSize, forward_max_Pooling_window_size)\n",
    "    elif forward_method == 'Combination':\n",
    "        numberWindows = posPool_combination(path, vectorSize, forward_max_Pooling_window_size,\n",
    "                                            forward_top_in_window)\n",
    "\n",
    "    creatFeatVector(path, numberWindows, vectorSize, primer_length)\n",
    "    getFeature(basic_path, path, variant_Name, Number=2000)\n",
    "\n",
    "    sameFeature(path)\n",
    "    calculateAppearance(basic_path, path, variant_Name)\n",
    "    commonFeatureAppearance(path)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "SHGHvkNWYEeW"
   },
   "outputs": [],
   "source": [
    "for variant_Name in variant_list:\n",
    "    old_path = basic_path + variant_Name + '/forward/' + 'Appearance_DataFrame.csv'\n",
    "    new_path = basic_path + variant_Name + '/reverse/forward_primer/' + 'Appearance_DataFrame.csv'\n",
    "    shutil.copyfile(old_path, new_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK0jF7-_YF6R"
   },
   "source": [
    "## reverse primer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aZyZwW0YH19",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variant_list = ['delta', 'omicron']  # Can change the order\n",
    "\n",
    "for variant_Name in variant_list:\n",
    "    path = basic_path + variant_Name + '/reverse/'\n",
    "\n",
    "    get_forward_primers(path)\n",
    "    CG_content_chect(path, min_CG=0.4, max_CG=0.6)\n",
    "\n",
    "    seq_file = S3_to_sageMaker('seq', variant_Name)\n",
    "    sequence_full = pd.read_csv(seq_file, header=None).values.ravel()  # get the sequence\n",
    "\n",
    "    primers = pd.read_csv(path + 'forward_primer_CG_check.csv', header=None).values.ravel()\n",
    "    files = glob.glob(path + 'result/*')\n",
    "    exist_primer_list = []\n",
    "    for file in files:\n",
    "        file = file.split('/')[-1].split('_')[0]\n",
    "        if file == 'temp':\n",
    "            pass\n",
    "        else:\n",
    "            exist_primer_list.append(file)\n",
    "            \n",
    "    temp_list = []\n",
    "    for forward_primer in primers:\n",
    "        if forward_primer not in exist_primer_list:\n",
    "            temp_list.append(forward_primer)\n",
    "            print(forward_primer)\n",
    "\n",
    "    primers = temp_list\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    for forward_primer in primers:\n",
    "        sequence_original = get_after_primer_data(path, forward_primer, sequence_full)\n",
    "        exist_primer_check(path, forward_primer, sequence_original)\n",
    "\n",
    "        count += 1\n",
    "        # file = path + 'second_data/' + forward_primer + '.csv'\n",
    "        print('No.{} file, the forward primer is    {}'.format(count, forward_primer))\n",
    "        # sequence_original = pd.read_csv(file).values.ravel()\n",
    "\n",
    "        rand = np.random.randint(100000)\n",
    "        np.random.seed(rand)\n",
    "        np.random.shuffle(sequence_original)\n",
    "\n",
    "\n",
    "        sequence = sequence_original[:train_model_number_reverse]\n",
    "        number = len(sequence)\n",
    "        random_sequence_T, random_sequence_V = generate_random_sequence(sequence, number)\n",
    "\n",
    "        seq_T = sequence\n",
    "        label_T = np.array([0 for x in range(len(seq_T))] + [1 for x in range(len(random_sequence_T))])\n",
    "        seq_V = sequence\n",
    "        label_V = np.array([0 for x in range(len(seq_V))] + [1 for x in range(len(random_sequence_V))])\n",
    "\n",
    "        seq_T = np.array(list(seq_T) + random_sequence_T)\n",
    "        seq_V = np.array(list(seq_V) + random_sequence_V)\n",
    "\n",
    "        rand = np.random.randint(100000)\n",
    "        np.random.seed(rand)\n",
    "        np.random.shuffle(seq_T)\n",
    "        np.random.seed(rand)\n",
    "        np.random.shuffle(label_T)\n",
    "\n",
    "        rand = np.random.randint(100000)\n",
    "        np.random.seed(rand)\n",
    "        np.random.shuffle(seq_V)\n",
    "        np.random.seed(rand)\n",
    "        np.random.shuffle(label_V)\n",
    "\n",
    "        '----------------------------------------------------------------------------------------------------------'\n",
    "\n",
    "        f = open(path + 'model/outputVector.txt', 'w')\n",
    "        f.write('1\\n')\n",
    "        f.write('1\\n')\n",
    "\n",
    "        # Parameters\n",
    "        kfoldIndex = 0\n",
    "        batchSize = reverse_batchSize\n",
    "\n",
    "        vectorSize = 0  # 31029 / 31079\n",
    "        for i in range(len(sequence_original)):\n",
    "            if len(sequence_original[i]) > vectorSize:\n",
    "                vectorSize = len(sequence_original[i])\n",
    "\n",
    "        if reverse_method == 'Top':\n",
    "            reverse_max_Pooling_window_size = int(vectorSize / reverse_top_number) + 1\n",
    "\n",
    "        labelSize = 2  # int(np.max(label_T) + 1)\n",
    "        limit = 1.01\n",
    "        iterMax = 1\n",
    "        # ------------------------------------------------\n",
    "        w1 = 12\n",
    "        wd1 = 21\n",
    "        h1 = reverse_max_Pooling_window_size  # 31029/148 ~ 210 ---> 31079/148 ~ 210\n",
    "        w4 = 256\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        # initialize variables\n",
    "        iter = 0\n",
    "        train_accuracy = 0.0\n",
    "        valid_accuracy = 0.0\n",
    "        test_accuracy = 0.0\n",
    "        # best validation accuracy\n",
    "        best = 0\n",
    "        validWindow = [0, 0, 0]\n",
    "        repeatWindow = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        validBest = 1e6\n",
    "        yResult = []\n",
    "        yTest = []\n",
    "\n",
    "        oneHot_labels_T = oneHot(label_T, labelSize)\n",
    "        oneHot_labels_V = oneHot(label_V, labelSize)\n",
    "        runs = int(len(oneHot_labels_T) / batchSize)\n",
    "\n",
    "        '----------------------------------------------------------------------------------------------------------'\n",
    "\n",
    "        # Tensorflow CNN model\n",
    "        print('\\nStrat to build the CNN model...')\n",
    "        tf.disable_v2_behavior()\n",
    "        sess = tf.InteractiveSession()\n",
    "\n",
    "        x = tf.placeholder(tf.float32, [None, vectorSize])  # input variable\n",
    "        keep_prob = tf.placeholder(tf.float32)  # keep between 0.50 to 1.0\n",
    "        y_ = tf.placeholder(tf.float32, [None, labelSize])  # expected outputs variable\n",
    "        x_image0 = tf.reshape(x, [-1, 1, vectorSize, 1])  # arrange the tensor as an image (1*30145) 1 channel\n",
    "        x_image = tf.transpose(x_image0, perm=[0, 3, 2, 1])  # arrange the tensor into 1 channels (1*30145)\n",
    "\n",
    "        # 1 LAYER\n",
    "        W_conv1 = weight_variable([1, wd1, 1, w1])\n",
    "        b_conv1 = bias_variable([w1])\n",
    "        h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
    "        h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 1, h1, 1], strides=[1, 1, h1, 1], padding='SAME')\n",
    "\n",
    "        # Rectifier LAYER\n",
    "        coef = int(\n",
    "            h_pool1.get_shape()[1] * h_pool1.get_shape()[2] * h_pool1.get_shape()[3])  # 1 * 209.34 * 12 ~ 2512\n",
    "        h_pool2_flat = tf.reshape(h_pool1, [-1, coef])\n",
    "        W_fc1 = weight_variable([coef, w4])\n",
    "        b_fc1 = bias_variable([w4])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "        # Rectifier-Dropout LAYER\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        W_fc2 = weight_variable([w4, labelSize])\n",
    "        b_fc2 = bias_variable([labelSize])\n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "        # Loss Function\n",
    "        cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_conv, labels=y_) + 0.001 * tf.nn.l2_loss(W_conv1))\n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "        trueResult = tf.argmax(y_conv, 1)\n",
    "        trueTest = tf.argmax(y_, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('\\nFinished the model building!')\n",
    "\n",
    "        '----------------------------------------------------------------------------------------------------------'\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        while ((best < limit) & (iter < iterMax)):\n",
    "            indexBatch = []\n",
    "            for iB in range(0, len(oneHot_labels_T)):\n",
    "                indexBatch.append(iB)\n",
    "                random.shuffle(indexBatch)\n",
    "            for run in range(0, runs):\n",
    "                xa, ya = getBatch_run(seq_T, oneHot_labels_T, batchSize, run, indexBatch, vectorSize)\n",
    "                train_step.run(feed_dict={x: xa, y_: ya, keep_prob: 0.5})\n",
    "\n",
    "            xa, ya = getBatch(seq_T, oneHot_labels_T, batchSize, vectorSize)\n",
    "            train_accuracy = accuracy.eval(feed_dict={x: xa, y_: ya, keep_prob: 1.0})\n",
    "\n",
    "            xaV, yaV = getBatch(seq_V, oneHot_labels_V, oneHot_labels_V.shape[0], vectorSize)\n",
    "            valid_accuracy = accuracy.eval(feed_dict={x: xaV, y_: yaV, keep_prob: 1.0})\n",
    "            cross_entropyVal = cross_entropy.eval(feed_dict={x: xaV, y_: yaV, keep_prob: 1.0})\n",
    "            cross_entropyTrain = cross_entropy.eval(feed_dict={x: xa, y_: ya, keep_prob: 1.0})\n",
    "\n",
    "            validWindowValue = 0\n",
    "            tempValid = validWindow\n",
    "            for i in range(0, len(validWindow) - 1):\n",
    "                tempValid[i] = validWindow[i + 1]\n",
    "            for i in range(0, len(validWindow)):\n",
    "                validWindow[i] = tempValid[i]\n",
    "            validWindow[len(validWindow) - 1] = valid_accuracy\n",
    "            for i in range(0, len(validWindow)):\n",
    "                validWindowValue = validWindowValue + validWindow[i]\n",
    "            validWindowValue = validWindowValue / len(validWindow)\n",
    "            tempValid = repeatWindow\n",
    "            for i in range(0, len(repeatWindow) - 1):\n",
    "                tempValid[i] = repeatWindow[i + 1]\n",
    "            for i in range(0, len(repeatWindow)):\n",
    "                repeatWindow[i] = tempValid[i]\n",
    "            repeatWindow[len(repeatWindow) - 1] = valid_accuracy\n",
    "            if np.var(repeatWindow) == 0 and iter > 10:\n",
    "                iter = iter\n",
    "            if validWindowValue > best or cross_entropyVal < validBest:\n",
    "                validBest = cross_entropyVal\n",
    "                best = validWindowValue\n",
    "\n",
    "                xaT, yaT = getBatch(seq_V, oneHot_labels_V, oneHot_labels_V.shape[0], vectorSize)\n",
    "                test_accuracy = accuracy.eval(feed_dict={x: xaT, y_: yaT, keep_prob: 1.0})\n",
    "                if kfoldIndex == 0:\n",
    "                    save_path = saver.save(sess, path + 'model/CNN_model.ckpt')\n",
    "\n",
    "                results = correct_prediction.eval(feed_dict={x: xaT, y_: yaT, keep_prob: 1.0})\n",
    "                yResult = trueResult.eval(feed_dict={x: xaT, y_: yaT, keep_prob: 1.0})\n",
    "                yTest = trueTest.eval(feed_dict={x: xaT, y_: yaT, keep_prob: 1.0})\n",
    "\n",
    "                fOut = open(path + 'model/outputVector_1.txt', 'w')\n",
    "                fOut.write('1\\n')\n",
    "                fOut.write('1\\n')\n",
    "                temp = 1.0 - best\n",
    "                trueAcc = str(temp)\n",
    "                print(trueAcc)\n",
    "                fOut.write(trueAcc + '\\n')\n",
    "                fOut.close()\n",
    "            log = \"%d\t%d\t%g\t%g\t%g\t%g\t%g\t%g\" % (\n",
    "                iter, kfoldIndex, train_accuracy, valid_accuracy, best,\n",
    "                test_accuracy, cross_entropyVal, cross_entropyTrain)\n",
    "            print(log)\n",
    "            f.write(log + '\\n')\n",
    "            iter = iter + 1\n",
    "\n",
    "        f.close()\n",
    "        np.savetxt(path + 'model/results.txt', yResult, fmt='%i', delimiter=' ')\n",
    "        np.savetxt(path + 'model/test.txt', yTest, fmt='%i', delimiter=' ')\n",
    "        f = open(path + 'model/log3.txt', 'a')\n",
    "        name = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' / CNN model'\n",
    "        f.write(name + '\\n')\n",
    "        f.close()\n",
    "\n",
    "        f = open(path + 'model/outputVector_2.txt', 'w')\n",
    "        f.write('1\\n')\n",
    "        f.write('1\\n')\n",
    "        temp = 1.0 - best\n",
    "        trueAcc = str(temp)\n",
    "        print(trueAcc)\n",
    "        f.write(trueAcc + '\\n')\n",
    "        f.close()\n",
    "\n",
    "        xaT, yaT = getBatch(seq_V, oneHot_labels_V, oneHot_labels_V.shape[0], vectorSize)\n",
    "\n",
    "        units = sess.run(h_conv1, feed_dict={x: xaT, y_: yaT, keep_prob: 1.0})\n",
    "        print(units.shape)\n",
    "        units = sess.run(h_pool1, feed_dict={x: xaT, y_: yaT, keep_prob: 1.0})\n",
    "        print(units.shape)\n",
    "\n",
    "        '----------------------------------------------------------------------------------------------------------'\n",
    "\n",
    "        sequence = sequence_original[train_model_number_reverse:generate_reverse_number + train_model_number_reverse]\n",
    "        seq_labels = np.array([0 for x in range(len(sequence))])\n",
    "\n",
    "        seq_count = 0\n",
    "        outData, outLabels = [], []\n",
    "        print('\\nStrat to transfer the sequence ...\\n')\n",
    "        for i in range(len(sequence)):\n",
    "            seq_count += 1\n",
    "            if seq_count % 100 == 0:\n",
    "                print('Transferring...  No.{} with total {}'.format(seq_count, len(seq_labels)))\n",
    "            sample = np.zeros(vectorSize)\n",
    "            for j in range(0, len(sequence[i])):\n",
    "                if sequence[i][j] == 'C':\n",
    "                    sample[j] = 0.25\n",
    "                elif sequence[i][j] == 'T':\n",
    "                    sample[j] = 0.50\n",
    "                elif sequence[i][j] == 'G':\n",
    "                    sample[j] = 0.75\n",
    "                elif sequence[i][j] == 'A':\n",
    "                    sample[j] = 1.0\n",
    "                else:\n",
    "                    sample[j] = 0.0\n",
    "            outData.append(sample)\n",
    "            outLabels.append(seq_labels[i])\n",
    "\n",
    "        print('\\nTransfer finished!\\n')\n",
    "        pd.DataFrame(sequence).to_csv(path + 'filter_seq/filter_seq.csv', header=None, index=None)\n",
    "        data = np.array(outData)\n",
    "        seq_labels = np.array(outLabels)\n",
    "        oneHotLabels = oneHot(seq_labels, labelSize)\n",
    "\n",
    "        '----------------------------------------------------------------------------------------------------------'\n",
    "        'Filter files'\n",
    "\n",
    "        print('the Filter files')\n",
    "        units = sess.run(h_conv1, feed_dict={x: data, y_: oneHotLabels, keep_prob: 1.0})\n",
    "        print('\\nThe first h_conv1 layer size:      h_conv1 = {}\\n'.format(units.shape))\n",
    "\n",
    "        sampleSize = int(data.shape[0])\n",
    "        Mat = np.zeros((sampleSize, vectorSize))\n",
    "\n",
    "        for filterIndex in range(units.shape[3]):\n",
    "            print('Loop 1 : Generating the  {}  Filter file'.format(filterIndex))\n",
    "            for testSize in range(sampleSize):\n",
    "                for inputSize in range(vectorSize):\n",
    "                    Mat[testSize][inputSize] = units[testSize][0][inputSize][filterIndex]\n",
    "            pd.DataFrame(Mat).to_csv(path + '/filter/filter_' + str(filterIndex) + '.csv', header=None, index=None)\n",
    "\n",
    "\n",
    "        '----------------------------------------------------------------------------------------------------------'\n",
    "        '----------------------------------------------------------------------------------------------------------'\n",
    "        '----------------------------------------------------------------------------------------------------------'\n",
    "\n",
    "        if reverse_method == 'Pooling':\n",
    "            numberWindows = posPool(path, vectorSize, reverse_max_Pooling_window_size)\n",
    "        elif reverse_method == 'Top':\n",
    "            numberWindows = posPool_top(path, vectorSize, reverse_max_Pooling_window_size)\n",
    "        elif reverse_method == 'Combination':\n",
    "            numberWindows = posPool_combination(path, vectorSize, reverse_max_Pooling_window_size,\n",
    "                                                reverse_top_in_window)\n",
    "\n",
    "        creatFeatVector(path, numberWindows, vectorSize, primer_length)\n",
    "        getFeature(basic_path, path, variant_Name, Number=100)\n",
    "\n",
    "        reverse_sameFeature(path)\n",
    "        p_value = reverse_CG_content_chect(path, min_CG=0.4, max_CG=0.6)\n",
    "\n",
    "        if p_value == 2:\n",
    "            pass\n",
    "        else:\n",
    "            p_value = reverse_calculateAppearance(basic_path, path, variant_Name)\n",
    "            if p_value == 2:\n",
    "                pass\n",
    "            else:\n",
    "                reverse_commonFeatureAppearance(path, forward_primer)\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luQIEeIgYrvm"
   },
   "source": [
    "## Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QheAA4BJYnxb"
   },
   "outputs": [],
   "source": [
    "for variant_Name in variant_list:\n",
    "    path = basic_path + variant_Name + '/reverse/'\n",
    "    \n",
    "    seq_file = S3_to_sageMaker('seq', variant_Name)\n",
    "    sequence_full = pd.read_csv(seq_file, header=None).values.ravel()  # get the sequence\n",
    "\n",
    "    get_available_primers(path)\n",
    "    length_of_amplicon(path, sequence_full)\n",
    "    reshape_file(path)\n",
    "\n",
    "    reverse(path)\n",
    "    primer_design_rules(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxqVhSc4YrEY"
   },
   "outputs": [],
   "source": [
    "for variant_Name in variant_list:\n",
    "    folder = os.path.exists(basic_path + variant_Name + '/reverse/' + 'amplicon_length/r_exist_primers_deltaG.csv')\n",
    "    if folder:\n",
    "        shutil.copyfile(basic_path + variant_Name + '/reverse/' + 'amplicon_length/r_exist_primers_deltaG.csv',\n",
    "                        basic_path + 'Final_result/exist_' + variant_Name + '_primers_result.csv')\n",
    "\n",
    "    folder = os.path.exists(basic_path + variant_Name + '/reverse/' + 'amplicon_length/r_new_primers_deltaG.csv')\n",
    "    if folder:\n",
    "        shutil.copyfile(basic_path + variant_Name + '/reverse/' + 'amplicon_length/r_new_primers_deltaG.csv',\n",
    "                        basic_path + 'Final_result/new_' + variant_Name + '_primers_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
